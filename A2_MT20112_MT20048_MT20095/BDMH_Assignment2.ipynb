{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0e3fdd9",
   "metadata": {},
   "source": [
    "# Load training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a4ccd2a7",
   "metadata": {
    "is_executing": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_train = pd.read_csv('../Dataset/RNA_Train.csv')\n",
    "df_test = pd.read_csv('../Dataset/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977d5279",
   "metadata": {},
   "source": [
    "# Map character to number (peptides encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8a8ae235",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def charToNumEncoding(peptide):\n",
    "    i = 0\n",
    "    for character in peptide:\n",
    "        mapping[character] = i\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e7e399",
   "metadata": {},
   "source": [
    "# Creating list of list of instances of training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "65e82997",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def traindataEncoding(data):\n",
    "    Features = []\n",
    "    row = len(data)\n",
    "    for i in range(row):\n",
    "        l = []\n",
    "        sequence = data['Sequence'][i]\n",
    "        for character in sequence:\n",
    "            l.append(mapping[character])\n",
    "        l.append(data['label'][i])\n",
    "        Features.append(l)\n",
    "    #print(Features)\n",
    "    return Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb40e6f",
   "metadata": {},
   "source": [
    "# Creating list of list of instances of testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f90840cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def testdataEncoding(data):\n",
    "    Features = []\n",
    "    row = len(data)\n",
    "    for i in range(row):\n",
    "        l = []\n",
    "        sequence = data['Sequence'][i]\n",
    "        for character in sequence:\n",
    "            l.append(mapping[character])\n",
    "        Features.append(l)\n",
    "    #print(Features)\n",
    "    return Features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53086fe7",
   "metadata": {},
   "source": [
    "# Data Imbalance handling using Random oversampling on minority class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "263612c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def Oversampling(x_train,y_train):\n",
    "    oversample = RandomOverSampler(sampling_strategy='minority')\n",
    "    X_resampled, y_resampled = oversample.fit_resample(x_train, y_train)\n",
    "    print(\"x-shape :\",X_resampled.shape,\"\\ny-shape :\", y_resampled.shape)\n",
    "    return X_resampled,y_resampled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f17d9e",
   "metadata": {},
   "source": [
    "# Data Imbalance handling using ADASYN(Adaptive Synthetic Oversampling) on minority class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5584bb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Oversampling2(x_train, y_train):\n",
    "    adasyn = ADASYN(sampling_strategy='minority')\n",
    "    X_resampled, y_resampled = adasyn.fit_resample(x_train, y_train)\n",
    "    print(\"x-shape :\", X_resampled.shape, \"\\ny-shape :\", y_resampled.shape)\n",
    "    return X_resampled, y_resampled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e5ea60",
   "metadata": {},
   "source": [
    "# RandomForest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c24dfb0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def RandomForest(x,y,test_df):\n",
    "    clf = RandomForestClassifier()\n",
    "    clf.fit(x, y)\n",
    "    y_pred = clf.predict(test_df)\n",
    "    return y_pred  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573f6418",
   "metadata": {},
   "source": [
    "# XGBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2fd684f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def XGBoost(x,y,test_df):\n",
    "    clf = XGBClassifier()\n",
    "    clf.fit(x, y)\n",
    "    y_pred = clf.predict(test_df)\n",
    "    return y_pred  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e261005",
   "metadata": {},
   "source": [
    "# Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f30c454b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "def SVM(x, y, test_df):\n",
    "    model = svm.SVC(C=0.1, gamma=1, kernel='rbf')\n",
    "    model.fit(x, y)\n",
    "    y_pred = model.predict(test_df)\n",
    "    \n",
    "    return y_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdb54a9",
   "metadata": {},
   "source": [
    "# LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f4844908",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LightGBM(x, y, test_df):\n",
    "    param_grid = {\n",
    "        'learning_rate': [0.05, 0.1, 0.2],\n",
    "        'n_estimators': [50, 100, 150],\n",
    "        'num_leaves': [31, 50, 100],\n",
    "        'boosting_type': ['gbdt', 'dart', 'goss']\n",
    "    }\n",
    "    clf = GridSearchCV(LGBMClassifier(), param_grid, refit=True, verbose=2)\n",
    "    clf.fit(x, y)\n",
    "    y_pred = clf.predict(test_df)\n",
    "    return y_pred "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a130955",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ea365214",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LogisticRegressionModel(x, y, test_df):\n",
    "    param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100], 'penalty': ['l1', 'l2']}\n",
    "    clf = GridSearchCV(LogisticRegression(), param_grid, refit=True, verbose=2)\n",
    "    clf.fit(x, y)\n",
    "    y_pred = clf.predict(test_df)\n",
    "    return y_pred "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf08c64",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Pipelining of RandomForest and XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "31576e22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def Pipelining(x,y,test_df):\n",
    "    model = [('lr', RandomForestClassifier()), ('XG', make_pipeline(StandardScaler(), XGBClassifier()))]\n",
    "    clf = StackingClassifier(estimators=model)\n",
    "    clf.fit(x, y)\n",
    "    y_pred = clf.predict(test_df)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0e0dd6ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def GenerateCSV(y_pred):\n",
    "    df3 = pd.read_csv('../Dataset/test.csv')\n",
    "    df3['label'] = y_pred\n",
    "    compress = dict(method='zip', archive_name='result.csv')\n",
    "    df3.to_csv('result.zip', index=False, compression=compress)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7b70d592",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'X': 0, 'M': 1, 'L': 2, 'Q': 3, 'V': 4, 'R': 5, 'A': 6, 'G': 7, 'T': 8, 'W': 9, 'F': 10, 'P': 11, 'S': 12, 'C': 13, 'N': 14, 'E': 15, 'K': 16, 'H': 17, 'I': 18, 'Y': 19, 'D': 20}\n"
     ]
    }
   ],
   "source": [
    "mapping = {}      # dictionary where key = character and value = numeric\n",
    "\n",
    "charToNumEncoding('XMLQVRAGTWFPSCNEKHIYD')     # RNA Sequence\n",
    "print(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "63818c8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(traindataEncoding(df_train))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b421ca3e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0   1   2   3   4   5   6   7   8   9   10  11  12  13  14  15  16  17\n",
       "0   0   0   0   0   0   0   0   0   1   2   3   2   4   5   6   7   6   0\n",
       "1   0   0   0   0   0   0   0   1   2   3   2   4   5   6   7   6   5   0\n",
       "2   0   0   0   0   0   0   1   2   3   2   4   5   6   7   6   5   8   0\n",
       "3   0   0   0   0   0   1   2   3   2   4   5   6   7   6   5   8   9   0\n",
       "4   0   0   0   0   1   2   3   2   4   5   6   7   6   5   8   9  10   0\n",
       "5   0   0   0   1   2   3   2   4   5   6   7   6   5   8   9  10   5   0\n",
       "6   0   0   1   2   3   2   4   5   6   7   6   5   8   9  10   5  11   0\n",
       "7   0   1   2   3   2   4   5   6   7   6   5   8   9  10   5  11  12   0\n",
       "8   1   2   3   2   4   5   6   7   6   5   8   9  10   5  11  12   7   0\n",
       "9   2   3   2   4   5   6   7   6   5   8   9  10   5  11  12   7  13   0"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f933b600",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0   1   2   3   4   5   6   7   8   9   10  11  12  13  14  15  16  17\n",
      "5299  20  20   2  15   7   4  20   7  20   8  15   6   4   5  18   6  12   0\n"
     ]
    }
   ],
   "source": [
    "print(train_df.loc[[5299]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7835343a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_df =  pd.DataFrame(testdataEncoding(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ef3258a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>20</td>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>20</td>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>20</td>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>20</td>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>20</td>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0   1   2   3   4   5   6   7   8   9   10  11  12  13  14  15  16\n",
       "0   0   0   0   0   0   0   0   0  12  15   4  12  20   8  14   2  19\n",
       "1   0   0   0   0   0   0   0  12  15   4  12  20   8  14   2  19  12\n",
       "2   0   0   0   0   0   0  12  15   4  12  20   8  14   2  19  12  11\n",
       "3   0   0   0   0   0  12  15   4  12  20   8  14   2  19  12  11  10\n",
       "4   0   0   0   0  12  15   4  12  20   8  14   2  19  12  11  10  16"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "75f05d97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_train = train_df.loc[:, :16]       # independent variable\n",
    "y_train = train_df[17]               # dependent variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fda281",
   "metadata": {},
   "source": [
    "# Checking data imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "dec1d5c0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "0    291963\n",
      "1     38899\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "count = y_train.value_counts()\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "87a56984",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA2wAAAHrCAYAAACzRh4YAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAABeOElEQVR4nO3df3xP9f//8fvLfg/bDPuVYaLJ79+zfihvy4gK/UDKSKRUb01+1Ts/+vEm6Z1+iHf1zur9fktT0TtCwijWaDYhRK0khorNz7Ht+f3Dd+ezl228NrMd3K6Xy7m01zmP1zmP8zqv03Z3zuv5chhjjAAAAAAAtlOlshsAAAAAABSPwAYAAAAANkVgAwAAAACbIrABAAAAgE0R2AAAAADApghsAAAAAGBTBDYAAAAAsCkCGwAAAADYFIENAAAAAGyKwAYA5/Dzzz/L4XDI4XDo5ptvrux2KkXB/tevX7+yW0El4TwAgMpDYAOuUJMmTbL+ABs0aFCZ15Oenq5JkyZp0qRJSkpKKrf+KlJCQoK1D4cPH67sdi5phd9XDodDn3/+udPyQYMGWctmz55dSV3+H86D/3MpnQeF32MFk6+vr5o0aaIxY8bo0KFDTvVJSUlOtWPGjHFanpCQYC3r169fsducOnWq0zqGDx9e6r6Tk5N1xx13KCgoSB4eHqpVq5aaNm2qe++9Vx988EGp1wfgyuBe2Q0AuLSlp6dr8uTJ1uNL8V/fExIStHr1aklnAkVAQIC1LDQ0VF999ZUkyd/fvzLau6S98MILuvXWWyu7jYuO86DynThxQtu2bdO2bdu0atUqffPNN3Jzcyu2dtasWRo3bpwCAwNdXv/Zgerjjz/WG2+8IXd31/6UWrFihbp166bc3Fxr3h9//KE//vhD33//vQ4cOKD+/fu73A+AKweBDcAl4fjx4/L19a3w7Xp5eemGG26o8O1eLtatW6eVK1fqL3/5S2W3clngPChq/vz5qlmzpr766itNnDhRkvTtt98qOTm5xJ6PHj2qGTNm6Nlnn3VpG9u2bdN3333nNO/333/Xl19+qW7durm0jgkTJlhh7ZFHHtFtt92m3Nxc/fTTT1q5cqVycnJcWg+AKw+3RAJwUvgWsTlz5mjGjBlq2LChvLy81LJlS61cudKqrV+/vgYPHmw9njx5svXcSZMmWfMzMjI0dOhQ1atXT15eXgoKClLfvn21bds2p20Xvi1p0qRJmj17tiIjI+Xh4aHExERJ0qhRo3TdddcpNDRUXl5eqlatmtq0aaPp06c7/ct1gW+++UZ33323wsLC5OnpqZCQEN16661KT0+3bpMquKogSREREVYPP//8c4mf3SnN61Tgo48+UrNmzeTt7a1mzZopMTHRaT0JCQnnPT6l2f/Cnz3buXOnbr/9dlWrVk2BgYEaPny4Tp486VT/+++/a+DAgfL391dAQIAGDhyo33///bw9nc/zzz/vUt2uXbs0ePBghYeHy9PTUzVr1tStt96qFStWONUVvr1t0KBBWrZsmdq3by9vb2/VrVtXr7322gX3zHlg7/OgsHbt2qlz586aMGGCmjdvbs3/9ddfz/m8119/XdnZ2S5to/DVtcK3TM6bN8/lPjdu3ChJCgwM1MyZM9WtWzf17NlTjz/+uBYuXKiPP/64yHO+++479e/fX6GhofL09NRVV12lBx98UHv27ClSm5aWpptvvlk+Pj6qU6eOJk+erC+//LLYW35vvvlmp+Nb4FzHwdVeyvKe+PXXX/Xoo4+qYcOG8vb2Vo0aNRQdHa0PP/ywTD2cOHFCo0ePVqNGjeTl5aWqVasqIiJCffr00YIFC4o9PoCtGQBXpIkTJxpJRpKJi4srdn6DBg2snwum6tWrmz///NMYY0y9evWKLC+YJk6caIwxJjU11QQEBBRbU61aNZOSkmJte86cOSVue86cOcYYY7y8vErc5uDBg5328d133zVubm7F1s6ZM8esWrWqxHVJMhkZGSYjI8N6fNNNN5XpdTLGmI8//tg4HI4idS1btiyyj+dSmv0vmO/n52dq1qxZpP7pp5+2anNyckzr1q2L1LRo0cL6uV69euftr/Dr0q5dO+vndevWGWOMiYuLs+bNmjXLel5KSoqpXr16sfvlcDjMm2++adUWPm716tUzVapUKfKc5cuXn7fXs/vlPLh0zoOz+yvQrFkza35SUpI1v/A+XnPNNdZ77YUXXijymvft27fI9ho2bGgkGXd3d5OZmWlq1aplnVsnT548b7/GGFO7dm1rG+PGjTObN282+fn5JdZ//vnnJR7nkJAQ89NPP1m1P/zwg/Hz8zvn61r4/X3TTTcV+/oVPp6Fj0NpeinteyItLc0EBgYWu+7CPZemhwceeKDE9/OAAQNcOl6AnXCFDUCJfvrpJ40dO1b/+9//1LJlS0nSkSNHNHfuXEln/qX8qaeesuoHDx6sr776Sl999ZUeeOABGWMUFxdnDWAwatQoffHFF3rxxRfl5uamo0ePavDgwTLGFLvt2NhYLVy4UImJiWratKkk6emnn9YHH3ygpUuXKikpSZ988omioqIknbkyUfAvrb/99psefvhh5eXlSZJ69eqlBQsW6KOPPtLQoUPl6emp1q1b66uvvlKrVq2s7c6fP9/ah9DQ0HJ5nfLy8jRy5EhrP++++24tXrxYjz/+uDZt2uTSNgq4uv+FZWdnq3bt2vr444/13HPPWfP/+c9/Wj/PmTNHaWlpkqSaNWvq3Xff1fz583X06NFS9VdYly5d1LFjR0ly2u7ZjDEaPHiwjhw5Ikm66667tHjxYj3zzDOqUqWKjDEaOXJksVdMfvnlF91222367LPPnK58FN63C8V5YL/zoLBvv/1WSUlJevbZZ7VlyxZJUpMmTUq8HbJGjRp65JFHJEmvvPKKjh07dt7179q1S5LUuXNnBQcHq1evXpLOnFtnD6xTkpiYGOvnqVOnqnnz5qpRo4Zuv/12ffjhh07H//jx44qLi1NOTo7c3d31wgsv6IsvvrAGS8nMzLT2QZKeeeYZ62ph69attXDhQr3++uvauXOnS72dS2l7Kex87wljjAYOHKg///xTktSsWTP9+9//1uLFizVhwgTVrFmzTD18+umnkqR69erpo48+0hdffKF//etfGjhwoGrUqHHBrwlQ4SorKQKoXK5cWbjjjjus+fPmzbPmjxw50ppf+F+mC64mFEhLS7OWtWrVynz11VfWFB0dbS379ttvi6yrXr165vTp00X6/vrrr80dd9xhQkJCjLu7e5F/Pf3000+NMca88sor1rzrrrvunK9FSf/abIxx6crC+V6nlJQUp38JPnXqlFXfsWPHUl1ZcHX/jXG+CpGWlmbNb9y4sTX/8OHDxhhjunfvbs2bOXOmVbt8+XKnY3I+hV+XsWPHms8++8zpOBd3hW3jxo0lvj533nmnteyVV14xxjhfLQkKCrKucGRmZjq93woUft8VTAXP4Tz4P5fSeXD2/hae+vTpY/bu3etUX/g9ExUVZfbv3298fX2NJDN9+vRzXmEbNWqUteyf//ynMcaYpUuXWvPuueee8/ZrjDG//fabadu27Tn7LrBgwQJrfvfu3Z3eM/Xr1zfSmSvPBw8eNHl5eaZatWpW/datW631PP3008W+v0tzha00vZy9jvO9JwqfG35+fubAgQPFvnal7SEkJMRIZ64wpqWluXwVFLArBh0BUKKbbrrJ+rngXzoluTzk9w8//GD9nJ6erhtvvLHYum3btqlt27ZO87p161Zk9LX169erc+fOOn36dInbLOit8LZ79OjhUr9ldb7X6aeffrLmtWnTRh4eHtbj6OhoffPNNy5tpzT7X5ifn5/T1ZOze/T393fqsX379tbPHTp0cKm3kvTs2VOtW7dWWlqann/++WJHGCx8rM5+fTp06GB9tqdwXYGOHTvKy8tLUsnv0eLedxkZGS5/rxzngWsq6jw4n2+//VZHjhw555XBoKAgDRs2TDNmzNDLL7+sCRMmFFtnjLE+R+Xm5qbevXtLOnP1ODAwUH/++acWLVqkY8eOqWrVqufsKywsTMnJyVqyZIkWLFig1atXKyMjw1r+ySef6IsvvlDXrl2djtuSJUu0ZMmSYnvbvn27GjZsaF0Jr1q1qpo0aWLVXOj5K6lUvZx9VfN874nC646KilLt2rXLpYchQ4bohRde0KZNm9S6dWu5ubnpmmuuUbdu3TR69GiXrxoDdsEtkQBKVPjWkcJ/NJpibt26EMXdkhQcHFxk3uzZs60/Unv27KnPP/9cX331lQYOHGjV5Ofnl2tvrijN6+RwOMq8nbLu/9m3AJXmWF5IvwWefvppSWduUyq4Zc1V59t+RbxHOQ9cU1HnwdkyMjJ04MAB3XXXXZKk3bt369577z3v8Rk9erS8vLy0b98+vfvuu8XWfP3119btpXl5eQoKCpLD4ZCHh4d1G9/x48etW/DOx8PDQ7fffrvmzJmjn376SZs3b1bjxo2t5QUDk7jqfLdzlvQ6F55fcLuspAsaZKi4Xirq3Dm7h+eee04ffPCB7r77bkVGRsrhcGjbtm165ZVX1LVr12IH5gHsjMAG4IJUqfJ//xs5+4/Ea665xvr5pptukjGmyHTs2DE99NBDRdZb3B8av/32m/XzlClT1L17d91www3av39/kdrC2z7fZ0zOtQ/l4eqrr7Z+TktLc/oDKTk52eX1lGb/S6tBgwbWz99++631c0pKygWvu0+fPmrSpImMMUpNTS2yvPCxSktLc/pjqvD2C9eVRnHvO1evrrmK8+D8yus8KE7t2rX19ttvWwEhNTX1vCEqLCzMGt1zw4YNxda4+mXWrowWuWTJkiKva7NmzdS9e3frccFrUvi4xcXFlfieiY2NVVBQkHV179ixY06jjpZ0/ha+0p2ZmSnpzDFfvnx5kdrS9FJahde9fv36EgNjWXro16+fEhMTtX37dh05csQK9Fu2bCn2aj1gZ9wSCeCCFP4X1KVLl6pTp07y9vZW8+bN1bJlSzVr1kxbtmzR6tWrNXDgQN19993y8PDQzz//rPXr12vBggU6dOiQS9uqV6+e9fOUKVMUFxenJUuWaNmyZUVq7777bo0bN045OTlau3at7rzzTg0cOND6o+T666/XgAEDiuzD22+/rVtvvVU+Pj5q165dWV8WJ23atFF4eLh+/fVX7d27VwMHDtSAAQO0bNmyUt0GVpr9L63bb7/dus1owoQJ8vHxUbVq1TR+/PgLXrfD4dBTTz2l++67r9jlrVq10rXXXqtt27Zp3759GjBggAYNGqSUlBRrCG5PT0/deeedF9zLxcJ5cH7ldR6UJCAgQA899JCmTp0qSZo2bZo1OEhJxo0bp3feeafYKy65ubn66KOPJJ15D0+fPl2enp5ONePHj9fRo0e1bNkyHTp06JwDWhQM8tK3b1916NBBAQEB2r59u+bMmWPVFNyOfMstt6h27do6ePCg3n//fQUGBuqWW25RXl6efv75Z61du1abNm3S999/rypVqqhnz57WrZv333+/nnnmGf3222+aMWNGsb00bNjQ+vmxxx7Tgw8+qEWLFhUbZErTS2kVPjeysrLUpUsXjRkzRoGBgUpNTdWhQ4f08ssvl7qH66+/Xq1bt1aHDh101VVX6ciRI0798Z13uORcpM/GAbA5VwZbKPzh/8If2i9cf/DgwWKHWl61apUx5tzDmRdMBc41cIMxZwYtOHtIcIfD4TRwQ+Ge33777WKHfD+77vXXXy+yvGCADVcGW3DldSppOPPmzZu7PNhCaff/7H0pUNyAAzk5OU5DgBdMjRo1KnE9xTl70JECubm51tDoBVN5DOtf+DU+1z670i/nwaVxHhhT8rD+v/32m/Hw8LCWff3110X6iYqKclrXoEGDnNZXMOhI4YFF2rZtW2wfvXr1smreeeedc/Z81VVXnfP4d+7c2WmY/8WLF5/z6xsKv8dLGta/8NdyFD4O33//fbHvicIDEhU+DqXppbTviXOdG4XrStPD1VdfXWJdkyZNTG5u7jmPFWA33BIJ4ILUqlVLCxcuVOvWreXj41NkeZs2bZSenq7hw4erQYMG8vT0VEBAgJo1a6bhw4cX+VLkc+nQoYMWLFig5s2by9vbW02bNtX8+fPVtWvXYusffPBBffXVV+rTp4+Cg4Pl7u6uoKAgde/e3WkQjoceekhjx45V3bp1nW4LK099+vRRYmKimjRpIk9PT1177bWaO3euunTpYtX4+vqecx2l3f/S8PT01PLlyzVgwAD5+fnJz89P99xzj5KSki543dKZARvOdbWuQ4cOSk1NVVxcnK666iq5u7urRo0a6tatm7744gs9/PDD5dLHxcJ54JryOA/OJSwsTP3797ceT5s27bzPeeqpp+Tm5lZkfuHbIW+//fZin3vbbbdZP5/vtsh58+Zp3Lhxio6OVp06deTp6SlfX1+1atVKL7zwgj7//HOnW2BvvfVWffvtt7r//vtVp04deXh4qFatWmrVqpXi4+M1f/58q7ZRo0ZatWqVOnXqJC8vL4WGhupvf/ubXn755WJ7ufbaa/Xf//5XDRs2lKenp/UF5n379i22vjS9lFabNm20adMmPfzww07nRseOHZ1uFy1ND+PHj9cdd9yhevXqydfXVx4eHqpfv76GDx+ulStXFnu8ATtzGHORPvkJALAYY4r9PFLHjh2tz5ls3LhRrVu3rujWgArDeVCxkpKS1LlzZ0lnPvuVkJBQuQ0BKBOusAFABfjqq6/Uv39/LVu2TL/88os2bdqkESNGWH+kRkZGWl8sC1yuOA8AoPQYdAQAKkB+fr7mzZtX7G1T1atXV0JCwkW7DQ2wC84DACg9/q8IABWgQYMGuu+++3T11VfL19dXXl5eatiwoR5++GFt2rRJHTt2rOwWgYuO8wAASo/PsAEAAACATXGFDQAAAABsisAGAAAAADZFYAMAAAAAmyKwAQAAAIBNEdgAAAAAwKYIbAAAAABgUwQ2AAAAALApAhsAAAAA2BSBDQAAAABsisAGAAAAADZFYAMAAAAAmyKwAQAAAIBNEdgAAAAAwKYIbAAAAABgUwQ2AAAAALApAhsAAAAA2BSBDQAAAABsisAGAAAAADZFYAMAAAAAmyKwAQAAAIBNEdgAAAAAwKYIbAAAAABgUwQ2AAAAALAp98pu4EqSn5+vvXv3qnr16nI4HJXdDgAAAIBKYozRkSNHFBYWpipVSr6ORmCrQHv37lV4eHhltwEAAADAJn799VfVqVOnxOUEtgpUvXp1SWcOip+fXyV3AwAAAKCyZGdnKzw83MoIJSGwVaCC2yD9/PwIbAAAAADO+1EpBh0BAAAAAJuq1MA2a9YstWjRwrriFB0drSVLlljLT548qREjRqhmzZqqVq2a7rzzTu3fv99pHbt371aPHj3k6+uroKAgjR49Wrm5uU41SUlJatOmjby8vNSwYUMlJCQU6WXmzJmqX7++vL29FRUVpfXr1zstd6UXAAAAAChPlRrY6tSpo6lTpyo1NVXffvut/vKXv+iOO+7Q1q1bJUlPPPGEPvvsM82fP1+rV6/W3r171adPH+v5eXl56tGjh06dOqV169bpvffeU0JCgiZMmGDVZGRkqEePHurcubPS09M1cuRIPfjgg1q2bJlV8+GHHyo+Pl4TJ07Uxo0b1bJlS8XGxurAgQNWzfl6AQAAAIDy5jDGmMpuorDAwEC99NJLuuuuu1S7dm3NnTtXd911lyRp+/btuvbaa5WcnKyOHTtqyZIl6tmzp/bu3avg4GBJ0uzZszV27FgdPHhQnp6eGjt2rBYvXqwtW7ZY2+jXr58OHz6spUuXSpKioqLUvn17vfHGG5LODL8fHh6uxx57TOPGjVNWVtZ5e3FFdna2/P39lZWVxWfYAAAAKoExRrm5ucrLy6vsVnCZc3Nzk7u7e4mfUXM1G9hm0JG8vDzNnz9fx44dU3R0tFJTU3X69GnFxMRYNY0bN1bdunWtkJScnKzmzZtbYU2SYmNj9fDDD2vr1q1q3bq1kpOTndZRUDNy5EhJ0qlTp5Samqrx48dby6tUqaKYmBglJydLkku9FCcnJ0c5OTnW4+zs7LK/QAAAALggp06d0r59+3T8+PHKbgVXCF9fX4WGhsrT07PM66j0wLZ582ZFR0fr5MmTqlatmhYsWKAmTZooPT1dnp6eCggIcKoPDg5WZmamJCkzM9MprBUsL1h2rprs7GydOHFChw4dUl5eXrE127dvt9Zxvl6KM2XKFE2ePNm1FwIAAAAXTX5+vjIyMuTm5qawsDB5enqed3Q+oKyMMTp16pQOHjyojIwMNWrU6Jxfjn0ulR7YIiMjlZ6erqysLH300UeKi4vT6tWrK7utcjF+/HjFx8dbjwu+awEAAAAV69SpU9bHXnx9fSu7HVwBfHx85OHhoV9++UWnTp2St7d3mdZT6YHN09NTDRs2lCS1bdtWGzZs0Kuvvqq+ffvq1KlTOnz4sNOVrf379yskJESSFBISUmQ0x4KRGwvXnD2a4/79++Xn5ycfHx+5ubnJzc2t2JrC6zhfL8Xx8vKSl5dXKV4NAAAAXExlvcoBlEV5vN9s947Nz89XTk6O2rZtKw8PD61YscJatmPHDu3evVvR0dGSpOjoaG3evNlpNMfly5fLz89PTZo0sWoKr6OgpmAdnp6eatu2rVNNfn6+VqxYYdW40gsAAAAAlLdKvcI2fvx4de/eXXXr1tWRI0c0d+5cJSUladmyZfL399eQIUMUHx+vwMBA+fn56bHHHlN0dLQ1yEfXrl3VpEkT3X///Zo2bZoyMzP1t7/9TSNGjLCubA0fPlxvvPGGxowZowceeEArV65UYmKiFi9ebPURHx+vuLg4tWvXTh06dNCMGTN07NgxDR48WJJc6gUAAAAAylulBrYDBw5o4MCB2rdvn/z9/dWiRQstW7ZMt9xyiyTplVdeUZUqVXTnnXcqJydHsbGxevPNN63nu7m5adGiRXr44YcVHR2tqlWrKi4uTs8++6xVExERocWLF+uJJ57Qq6++qjp16uidd95RbGysVdO3b18dPHhQEyZMUGZmplq1aqWlS5c6DURyvl4AAABwidq9W/r994rbXq1aUt26Fbc9FGvSpElauHCh0tPTK7uVc7Ld97BdzvgeNgAAgMpx8uRJZWRkKCIiwnnwh927pchI6eTJimvG21vascPl0DZo0CC99957mjJlisaNG2fNX7hwoXr37q2L/ef8oEGDdPjwYS1cuNDl5zgcDi1YsEC9evW6aH2VRnH9HD16VDk5OapZs+ZF226J7zu5ng1s9xk2AAAAoML8/nvFhjXpzPZKeUXP29tbL774og4dOnSRmrKn06dPX7R1V6tW7aKGtfJCYAMAAABsLiYmRiEhIZoyZco56z7++GM1bdpUXl5eql+/vl5++WWn5fXr19ff//53PfDAA6pevbrq1q2rt956q1S93HzzzXr88cc1ZswYBQYGKiQkRJMmTXLahiT17t1bDofDeixJn376qdq0aSNvb281aNBAkydPVm5urrXc4XBo1qxZuv3221W1alW98MILysvL05AhQxQRESEfHx9FRkbq1VdfLdLXu+++a+17aGioHn300XP2M2nSJLVq1cp6/qBBg9SrVy9Nnz5doaGhqlmzpkaMGOEUGvft26cePXrIx8dHERERmjt3rurXr68ZM2aU6jUsDQIbAAAAYHNubm76+9//rtdff1179uwptiY1NVX33HOP+vXrp82bN2vSpEl65plnlJCQ4FT38ssvq127dkpLS9Mjjzyihx9+WDt27ChVP++9956qVq2qlJQUTZs2Tc8++6yWL18uSdqwYYMkac6cOdq3b5/1+KuvvtLAgQP117/+Vd9//73++c9/KiEhQS+88ILTuidNmqTevXtr8+bNeuCBB5Sfn686depo/vz5+v777zVhwgQ99dRTSkxMtJ4za9YsjRgxQsOGDdPmzZv1v//9z/rqsJL6Kc6qVav0448/atWqVXrvvfeUkJDg9PoNHDhQe/fuVVJSkj7++GO99dZbTiPWXxQGFSYrK8tIMllZWZXdyhkSExMT0+U7AUAhJ06cMN9//705ceKE84LU1Mr5f1Rqqsu9x8XFmTvuuMMYY0zHjh3NAw88YIwxZsGCBabwn/P33nuvueWWW5yeO3r0aNOkSRPrcb169cx9991nPc7PzzdBQUFm1qxZLm3fGGNuuukmc8MNNzjVtG/f3owdO9Z6LMksWLDAqaZLly7m73//u9O8f//73yY0NNTpeSNHjiyxlwIjRowwd955p/U4LCzMPP300yXWF9fPxIkTTcuWLa3HcXFxpl69eiY3N9ead/fdd5u+ffsaY4zZtm2bkWQ2bNhgLd+5c6eRZF555ZVit1vi+864ng24wgYAAABcIl588UW999572rZtW5Fl27Zt0/XXX+807/rrr9fOnTuVl5dnzWvRooX1s8PhUEhISKmvEhVehySFhoaedx2bNm3Ss88+q2rVqlnT0KFDtW/fPh0/ftyqa9euXZHnzpw5U23btlXt2rVVrVo1vfXWW9q9e7ekMyPP7927V126dCnVPhSnadOmcnNzK3a/duzYIXd3d7Vp08Za3rBhQ9WoUeOCt3sulTqsPwAAAADXderUSbGxsRo/frwGDRpUpnV4eHg4PXY4HMrPz7/o6zh69KgmT56sPn36FFlWeATFqlWrOi2bN2+ennzySb388suKjo5W9erV9dJLLyklJUWS5OPjU6rez6U8XpvyRmADAAAALiFTp05Vq1atFBkZ6TT/2muv1dq1a53mrV27Vtdcc43TVaOK4OHh4XRVT5LatGmjHTt2WJ8tc9XatWt13XXX6ZFHHrHm/fjjj9bP1atXV/369bVixQp17tzZ5X5KKzIyUrm5uUpLS1Pbtm0lSbt27broI3cS2AAAAIBLSPPmzTVgwAC99tprTvNHjRql9u3b67nnnlPfvn2VnJysN954Q2+++WaF91gQoK6//np5eXmpRo0amjBhgnr27Km6devqrrvuUpUqVbRp0yZt2bJFzz//fInratSokd5//30tW7ZMERER+ve//60NGzYoIiLCqpk0aZKGDx+uoKAgde/eXUeOHNHatWv12GOPldhPaTVu3FgxMTEaNmyYZs2aJQ8PD40aNUo+Pj5yOBylf5FcxGfYAAAAcOWqVevMF1lXJG/vM9u9AM8++2yRW/XatGmjxMREzZs3T82aNdOECRP07LPPlvnWyQvx8ssva/ny5QoPD1fr1q0lSbGxsVq0aJG++OILtW/fXh07dtQrr7yievXqnXNdDz30kPr06aO+ffsqKipKf/zxh9PVNkmKi4vTjBkz9Oabb6pp06bq2bOndu7cec5+yuL9999XcHCwOnXqpN69e2vo0KGqXr16kS/FLk+O/z9qCiqAq99mXmEu4r8EAECl49cbgEJOnjypjIwMRUREFP3jevfuUn+R9QWpVUuqW7fitoeLZs+ePQoPD9eXX35Z7KAn53rfuZoNuCUSAAAAV7a6dQlQcMnKlSt19OhRNW/eXPv27dOYMWNUv359derU6aJtk8AGAAAAAC44ffq0nnrqKf3000+qXr26rrvuOv33v/8tMrpkeSKwAQAAAIALYmNjFRsbW6HbZNARAAAAALApAhsAAAAA2BSBDQAAAABsisAGAAAAADZFYAMAAAAAm2KUSAAAAFzRdmft1u/HK+6Ls2v51lJdf773Da4hsAEAAOCKtTtrtyLfiNTJ3JMVtk1vd2/teHTHFR3aJk2apIULFyo9Pb2yW7E9bokEAADAFev3479XaFiTpJO5J0t1RW/QoEHq1auXy/UOh0MLFy4sfWMXSXH9PPnkk1qxYkXlNHSJ4QobAAAAgCJOnz4tDw+Pi7LuatWqqVq1ahdl3ZcbrrABAAAAl4ibb75Zjz/+uMaMGaPAwECFhIRo0qRJ1vL69etLknr37i2Hw2E9lqRPP/1Ubdq0kbe3txo0aKDJkycrNzfXWu5wODRr1izdfvvtqlq1ql544QXl5eVpyJAhioiIkI+PjyIjI/Xqq68W6evdd99V06ZN5eXlpdDQUD366KPn7GfSpElq1aqV9fyCq4jTp09XaGioatasqREjRuj06dNWzb59+9SjRw/5+PgoIiJCc+fOVf369TVjxowLek3tjitsAAAAwCXkvffeU3x8vFJSUpScnKxBgwbp+uuv1y233KINGzYoKChIc+bMUbdu3eTm5iZJ+uqrrzRw4EC99tpruvHGG/Xjjz9q2LBhkqSJEyda6540aZKmTp2qGTNmyN3dXfn5+apTp47mz5+vmjVrat26dRo2bJhCQ0N1zz33SJJmzZql+Ph4TZ06Vd27d1dWVpbWrl0rSSX2U5xVq1YpNDRUq1at0q5du9S3b1+1atVKQ4cOlSQNHDhQv//+u5KSkuTh4aH4+HgdOHDgorzGdkJgAwAAAC4hLVq0sEJWo0aN9MYbb2jFihW65ZZbVLt2bUlSQECAQkJCrOdMnjxZ48aNU1xcnCSpQYMGeu655zRmzBinwHbvvfdq8ODBTtubPHmy9XNERISSk5OVmJhoBbbnn39eo0aN0l//+lerrn379pJUYj/FqVGjht544w25ubmpcePG6tGjh1asWKGhQ4dq+/bt+vLLL7Vhwwa1a9dOkvTOO++oUaNGpXjlLk0ENgAAAOAS0qJFC6fHoaGh573StGnTJq1du1YvvPCCNS8vL08nT57U8ePH5evrK0lWGCps5syZevfdd7V7926dOHFCp06dsm5nPHDggPbu3asuXbpc4F5JTZs2dboCFxoaqs2bN0uSduzYIXd3d7Vp08Za3rBhQ9WoUeOCt2t3BDYAAADgEnL2QCAOh0P5+fnnfM7Ro0c1efJk9enTp8gyb29v6+eqVas6LZs3b56efPJJvfzyy4qOjlb16tX10ksvKSUlRZLk4+NT1t0ooiz7dSUgsAEAAACXEQ8PD+Xl5TnNa9OmjXbs2KGGDRuWal1r167Vddddp0ceecSa9+OPP1o/V69eXfXr19eKFSvUuXNnl/sprcjISOXm5iotLU1t27aVJO3atUuHDh26oPVeCghsAAAAwGWkIEBdf/318vLyUo0aNTRhwgT17NlTdevW1V133aUqVapo06ZN2rJli55//vkS19WoUSO9//77WrZsmSIiIvTvf/9bGzZsUEREhFUzadIkDR8+XEFBQerevbuOHDmitWvX6rHHHiuxn9Jq3LixYmJiNGzYMM2aNUseHh4aNWqUfHx85HA4Sv8iXUIY1h8AAABXrFq+teTt7n3+wnLk7e6tWr61Ltr6X375ZS1fvlzh4eFq3bq1JCk2NlaLFi3SF198ofbt26tjx4565ZVXVK9evXOu66GHHlKfPn3Ut29fRUVF6Y8//nC62iZJcXFxmjFjht588001bdpUPXv21M6dO8/ZT1m8//77Cg4OVqdOndS7d28NHTpU1atXd7ql83LkMMaYym7iSpGdnS1/f39lZWXJz8+vstuRLvN/jQBwhePXG4BCTp48qYyMDEVERBT5A3931m79fvz3Cuullm8t1fWvW2Hbu1zt2bNH4eHh+vLLL8tl0JOL4VzvO1ezAbdEAgAA4IpW178uAeoSsHLlSh09elTNmzfXvn37NGbMGNWvX1+dOnWq7NYuKgIbAAAAANs7ffq0nnrqKf3000+qXr26rrvuOv33v/8tMrrk5YbABgAAAMD2YmNjFRsbW9ltVDgGHQEAAAAAmyKwAQAA4IrBeHuoSOXxfiOwAQAA4LJX8Dmn48ePV3InuJIUvN8u5HN2fIYNAAAAlz03NzcFBATowIEDkiRfX9/L/guXUXmMMTp+/LgOHDiggIAAubm5lXldBDYAAABcEUJCQiTJCm3AxRYQEGC978qKwAYAAIArgsPhUGhoqIKCgnT69OnKbgeXOQ8Pjwu6slaAwAYAAIAripubW7n8IQ1UBAYdAQAAAACbIrABAAAAgE0R2AAAAADApghsAAAAAGBTBDYAAAAAsCkCGwAAAADYFIENAAAAAGyKwAYAAAAANkVgAwAAAACbIrABAAAAgE0R2AAAAADApio1sE2ZMkXt27dX9erVFRQUpF69emnHjh1ONTfffLMcDofTNHz4cKea3bt3q0ePHvL19VVQUJBGjx6t3Nxcp5qkpCS1adNGXl5eatiwoRISEor0M3PmTNWvX1/e3t6KiorS+vXrnZafPHlSI0aMUM2aNVWtWjXdeeed2r9/f/m8GAAAAABwlkoNbKtXr9aIESP0zTffaPny5Tp9+rS6du2qY8eOOdUNHTpU+/bts6Zp06ZZy/Ly8tSjRw+dOnVK69at03vvvaeEhARNmDDBqsnIyFCPHj3UuXNnpaena+TIkXrwwQe1bNkyq+bDDz9UfHy8Jk6cqI0bN6ply5aKjY3VgQMHrJonnnhCn332mebPn6/Vq1dr79696tOnz0V8hQAAAABcyRzGGFPZTRQ4ePCggoKCtHr1anXq1EnSmStsrVq10owZM4p9zpIlS9SzZ0/t3btXwcHBkqTZs2dr7NixOnjwoDw9PTV27FgtXrxYW7ZssZ7Xr18/HT58WEuXLpUkRUVFqX379nrjjTckSfn5+QoPD9djjz2mcePGKSsrS7Vr19bcuXN11113SZK2b9+ua6+9VsnJyerYseN59y87O1v+/v7KysqSn59fmV+ncuNwVHYHAHDx2OfXGwAARbiaDWz1GbasrCxJUmBgoNP8//73v6pVq5aaNWum8ePH6/jx49ay5ORkNW/e3AprkhQbG6vs7Gxt3brVqomJiXFaZ2xsrJKTkyVJp06dUmpqqlNNlSpVFBMTY9Wkpqbq9OnTTjWNGzdW3bp1rZqz5eTkKDs722kCAAAAAFe5V3YDBfLz8zVy5Ehdf/31atasmTX/3nvvVb169RQWFqbvvvtOY8eO1Y4dO/TJJ59IkjIzM53CmiTrcWZm5jlrsrOzdeLECR06dEh5eXnF1mzfvt1ah6enpwICAorUFGznbFOmTNHkyZNL+UoAAAAAwBm2CWwjRozQli1b9PXXXzvNHzZsmPVz8+bNFRoaqi5duujHH3/U1VdfXdFtlsr48eMVHx9vPc7OzlZ4eHgldgQAAADgUmKLWyIfffRRLVq0SKtWrVKdOnXOWRsVFSVJ2rVrlyQpJCSkyEiNBY9DQkLOWePn5ycfHx/VqlVLbm5uxdYUXsepU6d0+PDhEmvO5uXlJT8/P6cJAAAAAFxVqYHNGKNHH31UCxYs0MqVKxUREXHe56Snp0uSQkNDJUnR0dHavHmz02iOy5cvl5+fn5o0aWLVrFixwmk9y5cvV3R0tCTJ09NTbdu2darJz8/XihUrrJq2bdvKw8PDqWbHjh3avXu3VQMAAAAA5alSb4kcMWKE5s6dq08//VTVq1e3Pgvm7+8vHx8f/fjjj5o7d65uvfVW1axZU999952eeOIJderUSS1atJAkde3aVU2aNNH999+vadOmKTMzU3/72980YsQIeXl5SZKGDx+uN954Q2PGjNEDDzyglStXKjExUYsXL7Z6iY+PV1xcnNq1a6cOHTpoxowZOnbsmAYPHmz1NGTIEMXHxyswMFB+fn567LHHFB0d7dIIkQAAAABQaqYSSSp2mjNnjjHGmN27d5tOnTqZwMBA4+XlZRo2bGhGjx5tsrKynNbz888/m+7duxsfHx9Tq1YtM2rUKHP69GmnmlWrVplWrVoZT09P06BBA2sbhb3++uumbt26xtPT03To0MF88803TstPnDhhHnnkEVOjRg3j6+trevfubfbt2+fy/mZlZRlJRfqvNGcGvWZiYmK6PCcAAGzM1Wxgq+9hu9zxPWwAUIH49QYAsLFL8nvYAAAAAAD/h8AGAAAAADZFYAMAAAAAmyKwAQAAAIBNEdgAAAAAwKYIbAAAAABgUwQ2AAAAALApAhsAAAAA2BSBDQAAAABsisAGAAAAADZFYAMAAAAAmyKwAQAAAIBNEdgAAAAAwKYIbAAAAABgUwQ2AAAAALApAhsAAAAA2BSBDQAAAABsisAGAAAAADZFYAMAAAAAmyKwAQAAAIBNEdgAAAAAwKYIbAAAAABgUwQ2AAAAALApAhsAAAAA2BSBDQAAAABsisAGAAAAADZFYAMAAAAAmyKwAQAAAIBNEdgAAAAAwKYIbAAAAABgUwQ2AAAAALApAhsAAAAA2BSBDQAAAABsisAGAAAAADZFYAMAAAAAmyKwAQAAAIBNEdgAAAAAwKYIbAAAAABgUwQ2AAAAALApAhsAAAAA2BSBDQAAAABsisAGAAAAADZFYAMAAAAAmyKwAQAAAIBNEdgAAAAAwKYIbAAAAABgUwQ2AAAAALApAhsAAAAA2BSBDQAAAABsisAGAAAAADZFYAMAAAAAm6rUwDZlyhS1b99e1atXV1BQkHr16qUdO3Y41Zw8eVIjRoxQzZo1Va1aNd15553av3+/U83u3bvVo0cP+fr6KigoSKNHj1Zubq5TTVJSktq0aSMvLy81bNhQCQkJRfqZOXOm6tevL29vb0VFRWn9+vWl7gUAAAAAykulBrbVq1drxIgR+uabb7R8+XKdPn1aXbt21bFjx6yaJ554Qp999pnmz5+v1atXa+/everTp4+1PC8vTz169NCpU6e0bt06vffee0pISNCECROsmoyMDPXo0UOdO3dWenq6Ro4cqQcffFDLli2zaj788EPFx8dr4sSJ2rhxo1q2bKnY2FgdOHDA5V4AAAAAoFwZGzlw4ICRZFavXm2MMebw4cPGw8PDzJ8/36rZtm2bkWSSk5ONMcZ8/vnnpkqVKiYzM9OqmTVrlvHz8zM5OTnGGGPGjBljmjZt6rStvn37mtjYWOtxhw4dzIgRI6zHeXl5JiwszEyZMsXlXs4nKyvLSDJZWVku1V90EhMTE9PlOwEAYGOuZgNbfYYtKytLkhQYGChJSk1N1enTpxUTE2PVNG7cWHXr1lVycrIkKTk5Wc2bN1dwcLBVExsbq+zsbG3dutWqKbyOgpqCdZw6dUqpqalONVWqVFFMTIxV40ovAAAAAFCe3Cu7gQL5+fkaOXKkrr/+ejVr1kySlJmZKU9PTwUEBDjVBgcHKzMz06opHNYKlhcsO1dNdna2Tpw4oUOHDikvL6/Ymu3bt7vcy9lycnKUk5NjPc7Ozj7fywAAAAAAFttcYRsxYoS2bNmiefPmVXYr5WbKlCny9/e3pvDw8MpuCQAAAMAlxBaB7dFHH9WiRYu0atUq1alTx5ofEhKiU6dO6fDhw071+/fvV0hIiFVz9kiNBY/PV+Pn5ycfHx/VqlVLbm5uxdYUXsf5ejnb+PHjlZWVZU2//vqrC68GAAAAAJxRqYHNGKNHH31UCxYs0MqVKxUREeG0vG3btvLw8NCKFSuseTt27NDu3bsVHR0tSYqOjtbmzZudRnNcvny5/Pz81KRJE6um8DoKagrW4enpqbZt2zrV5Ofna8WKFVaNK72czcvLS35+fk4TAAAAALiqUj/DNmLECM2dO1effvqpqlevbn0WzN/fXz4+PvL399eQIUMUHx+vwMBA+fn56bHHHlN0dLQ6duwoSeratauaNGmi+++/X9OmTVNmZqb+9re/acSIEfLy8pIkDR8+XG+88YbGjBmjBx54QCtXrlRiYqIWL15s9RIfH6+4uDi1a9dOHTp00IwZM3Ts2DENHjzY6ul8vQAAAABAuaqYQSuLJ6nYac6cOVbNiRMnzCOPPGJq1KhhfH19Te/evc2+ffuc1vPzzz+b7t27Gx8fH1OrVi0zatQoc/r0aaeaVatWmVatWhlPT0/ToEEDp20UeP31103dunWNp6en6dChg/nmm2+clrvSy7kwrD8TExNTBU4AANiYq9nAYYwxlRcXryzZ2dny9/dXVlaWPW6PdDgquwMAuHj49QYAsDFXs4EtBh0BAAAAABRFYAMAAAAAmyKwAQAAAIBNEdgAAAAAwKYIbAAAAABgUwQ2AAAAALApAhsAAAAA2BSBDQAAAABsisAGAAAAADZFYAMAAAAAmyKwAQAAAIBNEdgAAAAAwKYIbAAAAABgUwQ2AAAAALApAhsAAAAA2BSBDQAAAABsisAGAAAAADZFYAMAAAAAmyKwAQAAAIBNEdgAAAAAwKYIbAAAAABgUwQ2AAAAALApAhsAAAAA2BSBDQAAAABsisAGAAAAADZFYAMAAAAAmyKwAQAAAIBNEdgAAAAAwKYIbAAAAABgUwQ2AAAAALApAhsAAAAA2BSBDQAAAABsqkyBrUGDBrrrrruKzH/66afVt2/fC24KAAAAACC5l+VJP//8s0JCQorM//LLL/Xtt99ecFMAAAAAgFIGtvfff9/6+eDBg06Pjx07pm3btsnT07P8ugMAAACAK5jDGGNcLa5SpYocDkeJy40xatGihdLT08ujt8tOdna2/P39lZWVJT8/v8puRzrHsQSAS57rv94AAKhwrmaDUt8SaYyRw+HQ2TnPx8dHjRs31muvvVb6bgEAAAAARZQqsOXn50s6c6WtY8eOWrdu3UVpCgAAAABQxkFHVq1aZY9b+gAAAADgMlamwHbTTTfphx9+0FtvvaX9+/cXuT1ywoQJ5dIcAAAAAFzJSjXoSIF3331XDz30kHWL5Nny8vIuuLHLEYOOAEAFYtARAICNXbRBRyTp+eefJ5QBAAAAwEVWpSxP2r9/v/z9/bVp0yadPn1a+fn5ThMAAAAA4MKVKbB17txZgYGBat68udzc3Mq7JwAAAACAynhL5N13361hw4apX79+uvfeexUQEOC0vFOnTuXRGwAAAABc0co06EiVKlXkKGHACofDodzc3Atu7HLEoCMAUIEYdAQAYGMXddARSUWG8gcAAAAAlK8yBbaMjIzy7gMAAAAAcJYyBbZ69eqVdx8AAAAAgLOUKbA98MADJS5zOBz617/+VeaGAAAAAABnlOugI8YYORwOvlS7BAw6AgAViM9aAwBs7KIOOtKpUyenwJaVlaXNmzfLGKMbb7yxLKsEAAAAAJylTF+cnZSUpFWrVlnTxo0btXnzZvn5+alnz54ur2fNmjW67bbbFBYWJofDoYULFzotHzRokBwOh9PUrVs3p5o///xTAwYMkJ+fnwICAjRkyBAdPXrUqea7777TjTfeKG9vb4WHh2vatGlFepk/f74aN24sb29vNW/eXJ9//rnTcmOMJkyYoNDQUPn4+CgmJkY7d+50eV8BAAAAoLTKFNiK07hxY7Vq1Uqvv/66y885duyYWrZsqZkzZ5ZY061bN+3bt8+aPvjgA6flAwYM0NatW7V8+XItWrRIa9as0bBhw6zl2dnZ6tq1q+rVq6fU1FS99NJLmjRpkt566y2rZt26derfv7+GDBmitLQ09erVS7169dKWLVusmmnTpum1117T7NmzlZKSoqpVqyo2NlYnT550eX8BAAAAoDTK9Bm2999/3+lxXl6efvjhB02fPl2+vr7KysoqfSMOhxYsWKBevXpZ8wYNGqTDhw8XufJWYNu2bWrSpIk2bNigdu3aSZKWLl2qW2+9VXv27FFYWJhmzZqlp59+WpmZmfL09JQkjRs3TgsXLtT27dslSX379tWxY8e0aNEia90dO3ZUq1atNHv2bBljFBYWplGjRunJJ5+UdOY20ODgYCUkJKhfv34u7SOfYQOACsRn2AAANnZRP8NWcKvi2Ywxuummm8qyyhIlJSUpKChINWrU0F/+8hc9//zzqlmzpiQpOTlZAQEBVliTpJiYGFWpUkUpKSnq3bu3kpOT1alTJyusSVJsbKxefPFFHTp0SDVq1FBycrLi4+OdthsbG2sFxYyMDGVmZiomJsZa7u/vr6ioKCUnJ5cY2HJycpSTk2M9zs7OvuDXAwAAAMCVo8y3RBpjnKbatWurf//+evvtt8utuW7duun999/XihUr9OKLL2r16tXq3r27NQplZmamgoKCnJ7j7u6uwMBAZWZmWjXBwcFONQWPz1dTeHnh5xVXU5wpU6bI39/fmsLDw0u1/wAAAACubGW6wpafn1/efRSr8JWr5s2bq0WLFrr66quVlJSkLl26VEgPF2L8+PFOV+6ys7MJbQAAAABcdkGDjpw8eVKpqalKTU2tkME3GjRooFq1amnXrl2SpJCQEB04cMCpJjc3V3/++adCQkKsmv379zvVFDw+X03h5YWfV1xNcby8vOTn5+c0AQAAAICryhzY/v73v6tWrVrq0KGDOnTooFq1amnq1Knl2VsRe/bs0R9//KHQ0FBJUnR0tA4fPqzU1FSrZuXKlcrPz1dUVJRVs2bNGp0+fdqqWb58uSIjI1WjRg2rZsWKFU7bWr58uaKjoyVJERERCgkJcarJzs5WSkqKVQMAAAAA5c6Uwb/+9S/jcDiKTFWqVDFz5sxxeT1HjhwxaWlpJi0tzUgy//jHP0xaWpr55ZdfzJEjR8yTTz5pkpOTTUZGhvnyyy9NmzZtTKNGjczJkyetdXTr1s20bt3apKSkmK+//to0atTI9O/f31p++PBhExwcbO6//36zZcsWM2/ePOPr62v++c9/WjVr16417u7uZvr06Wbbtm1m4sSJxsPDw2zevNmqmTp1qgkICDCffvqp+e6778wdd9xhIiIizIkTJ1ze36ysLCPJZGVlufyci+rMGGpMTExMl+cEAICNuZoNyvQbrXXr1sbhcJg+ffqYxMREk5iYaHr37m0cDodp06aNy+tZtWqVkVRkiouLM8ePHzddu3Y1tWvXNh4eHqZevXpm6NChJjMz02kdf/zxh+nfv7+pVq2a8fPzM4MHDzZHjhxxqtm0aZO54YYbjJeXl7nqqqvM1KlTi/SSmJhorrnmGuPp6WmaNm1qFi9e7LQ8Pz/fPPPMMyY4ONh4eXmZLl26mB07dpTiVSOwMTExMVXoBACAjbmaDcr0PWw+Pj4KDQ3VTz/95DQ/IiJC+/fv1/Hjxy/wut/lie9hA4AKVPpfbwAAVBhXs0GZPsPm7u6ukydPKjc315p3+vRpnTx5Um5ubmVZJQAAAADgLGUa1r9Vq1Zat26dOnXqpD59+kiSPvnkEx04cEDXX399uTYIAAAAAFeqMgW20aNHq1evXkpJSVFKSookqeDOyjFjxpRfdwAAAABwBSvTLZG333673n//fYWHh8sYI2OM6tatq//85z/q2bNnefcIAAAAAFekUl1h+/nnn7VmzRpFRkbqvvvu03333aeDBw9Kknbt2qWdO3fq559/Vv369S9GrwAAAABwRSnVFbapU6dq8ODBTl9CXbt2bdWuXVvHjx/X4MGDL/qXZwMAAADAlaJUw/pHRkbqwIEDOnToULHLa9asqcDAQO3cubPcGrycMKw/AFQghvUHANjYRRnWf8+ePapbt26Jy8PDw/Xbb7+VZpUAAAAAgBKUKrC5u7vrl19+UX5+fpFleXl5+vnnn+Xh4VFuzQEAAADAlaxUge3aa6/VkSNH9PTTTxdZ9swzzyg7O1vXXnttuTUHAAAAAFeyUo0Sec8992j9+vWaNm2ali1bphtvvFEOh0Nff/210tLS5HA41Ldv34vVKwAAAABcUUo16EhOTo6io6OVnp4ux1kDVhhj1Lp1ayUnJ8vT07PcG70cMOgIAFQgBh0BANjYRRl0xMvLSytXrlT//v3l5uZmfWm2m5ub7r33Xn355ZeENQAAAAAoJ6W6wlZYdna2fvjhBxljFBkZaY8rRjbHFTYAqEBcYQMA2Jir2aBUn2ErzM/PT+3atSvr0wEAAAAA51GqWyIBAAAAABWHwAYAAAAANkVgAwAAAACbIrABAAAAgE0R2AAAAADApghsAAAAAGBTBDYAAAAAsCkCGwAAAADYFIENAAAAAGyKwAYAAAAANkVgAwAAAACbIrABAAAAgE0R2AAAAADApghsAAAAAGBTBDYAAAAAsCkCGwAAAADYFIENAAAAAGyKwAYAAAAANkVgAwAAAACbIrABAAAAgE0R2AAAAADApghsAAAAAGBTBDYAAAAAsCkCGwAAAADYFIENAAAAAGyKwAYAAAAANkVgAwAAAACbIrABAAAAgE0R2AAAAADApghsAAAAAGBTBDYAAAAAsCkCGwAAAADYFIENAAAAAGyKwAYAAAAANkVgAwAAAACbIrABAAAAgE0R2AAAAADApio1sK1Zs0a33XabwsLC5HA4tHDhQqflxhhNmDBBoaGh8vHxUUxMjHbu3OlU8+eff2rAgAHy8/NTQECAhgwZoqNHjzrVfPfdd7rxxhvl7e2t8PBwTZs2rUgv8+fPV+PGjeXt7a3mzZvr888/L3UvAAAAAFCeKjWwHTt2TC1bttTMmTOLXT5t2jS99tprmj17tlJSUlS1alXFxsbq5MmTVs2AAQO0detWLV++XIsWLdKaNWs0bNgwa3l2dra6du2qevXqKTU1VS+99JImTZqkt956y6pZt26d+vfvryFDhigtLU29evVSr169tGXLllL1AgAAAADlytiEJLNgwQLrcX5+vgkJCTEvvfSSNe/w4cPGy8vLfPDBB8YYY77//nsjyWzYsMGqWbJkiXE4HOa3334zxhjz5ptvmho1apicnByrZuzYsSYyMtJ6fM8995gePXo49RMVFWUeeughl3txRVZWlpFksrKyXH7ORSUxMTExXb4TAAA25mo2sO1n2DIyMpSZmamYmBhrnr+/v6KiopScnCxJSk5OVkBAgNq1a2fVxMTEqEqVKkpJSbFqOnXqJE9PT6smNjZWO3bs0KFDh6yawtspqCnYjiu9FCcnJ0fZ2dlOEwAAAAC4yraBLTMzU5IUHBzsND84ONhalpmZqaCgIKfl7u7uCgwMdKopbh2Ft1FSTeHl5+ulOFOmTJG/v781hYeHn2evAQAAAOD/2DawXQ7Gjx+vrKwsa/r1118ruyUAAAAAlxDbBraQkBBJ0v79+53m79+/31oWEhKiAwcOOC3Pzc3Vn3/+6VRT3DoKb6OkmsLLz9dLcby8vOTn5+c0AQAAAICrbBvYIiIiFBISohUrVljzsrOzlZKSoujoaElSdHS0Dh8+rNTUVKtm5cqVys/PV1RUlFWzZs0anT592qpZvny5IiMjVaNGDaum8HYKagq240ovAAAAAFDeKjWwHT16VOnp6UpPT5d0ZnCP9PR07d69Ww6HQyNHjtTzzz+v//3vf9q8ebMGDhyosLAw9erVS5J07bXXqlu3bho6dKjWr1+vtWvX6tFHH1W/fv0UFhYmSbr33nvl6empIUOGaOvWrfrwww/16quvKj4+3urjr3/9q5YuXaqXX35Z27dv16RJk/Ttt9/q0UcflSSXegEAAACAcldBo1YWa9WqVUZSkSkuLs4Yc2Y4/WeeecYEBwcbLy8v06VLF7Njxw6ndfzxxx+mf//+plq1asbPz88MHjzYHDlyxKlm06ZN5oYbbjBeXl7mqquuMlOnTi3SS2JiornmmmuMp6enadq0qVm8eLHTcld6OR+G9WdiYmKqwAkAABtzNRs4jDGmEvPiFSU7O1v+/v7Kysqyx+fZHI7K7gAALh5+vQEAbMzVbGDbz7ABAAAAwJWOwAYAAAAANkVgAwAAAACbIrABAAAAgE0R2AAAAADApghsAAAAAGBTBDYAAAAAsCkCGwAAAADYFIENAAAAAGyKwAYAAAAANkVgAwAAAACbIrABAAAAgE0R2AAAAADApghsAAAAAGBTBDYAAAAAsCkCGwAAAADYFIENAAAAAGyKwAYAAAAANkVgAwAAAACbIrABAAAAgE0R2AAAAADApghsAAAAAGBTBDYAAAAAsCkCGwAAAADYFIENAAAAAGyKwAYAAAAANkVgAwAAAACbIrABAAAAgE0R2AAAAADApghsAAAAAGBTBDYAAAAAsCkCGwAAAADYFIENAAAAAGyKwAYAAAAANkVgAwAAAACbIrABAAAAgE0R2AAAAADApghsAAAAAGBTBDYAAAAAsCkCGwAAAADYFIENAAAAAGyKwAYAAAAANkVgAwAAAACbIrABAAAAgE0R2AAAAADApghsAAAAAGBTBDYAAAAAsCkCGwAAAADYFIENAAAAAGyKwAYAAAAANkVgAwAAAACbIrABAAAAgE3ZOrBNmjRJDofDaWrcuLG1/OTJkxoxYoRq1qypatWq6c4779T+/fud1rF792716NFDvr6+CgoK0ujRo5Wbm+tUk5SUpDZt2sjLy0sNGzZUQkJCkV5mzpyp+vXry9vbW1FRUVq/fv1F2WcAAAAAKGDrwCZJTZs21b59+6zp66+/tpY98cQT+uyzzzR//nytXr1ae/fuVZ8+fazleXl56tGjh06dOqV169bpvffeU0JCgiZMmGDVZGRkqEePHurcubPS09M1cuRIPfjgg1q2bJlV8+GHHyo+Pl4TJ07Uxo0b1bJlS8XGxurAgQMV8yIAAAAAuCI5jDGmspsoyaRJk7Rw4UKlp6cXWZaVlaXatWtr7ty5uuuuuyRJ27dv17XXXqvk5GR17NhRS5YsUc+ePbV3714FBwdLkmbPnq2xY8fq4MGD8vT01NixY7V48WJt2bLFWne/fv10+PBhLV26VJIUFRWl9u3b64033pAk5efnKzw8XI899pjGjRvn8v5kZ2fL399fWVlZ8vPzK+vLUn4cjsruAAAuHvv+egMAwOVsYPsrbDt37lRYWJgaNGigAQMGaPfu3ZKk1NRUnT59WjExMVZt48aNVbduXSUnJ0uSkpOT1bx5cyusSVJsbKyys7O1detWq6bwOgpqCtZx6tQppaamOtVUqVJFMTExVk1JcnJylJ2d7TQBAAAAgKtsHdiioqKUkJCgpUuXatasWcrIyNCNN96oI0eOKDMzU56engoICHB6TnBwsDIzMyVJmZmZTmGtYHnBsnPVZGdn68SJE/r999+Vl5dXbE3BOkoyZcoU+fv7W1N4eHipXwMAAAAAVy73ym7gXLp372793KJFC0VFRalevXpKTEyUj49PJXbmmvHjxys+Pt56nJ2dTWgDAAAA4DJbX2E7W0BAgK655hrt2rVLISEhOnXqlA4fPuxUs3//foWEhEiSQkJCiowaWfD4fDV+fn7y8fFRrVq15ObmVmxNwTpK4uXlJT8/P6cJAAAAAFx1SQW2o0eP6scff1RoaKjatm0rDw8PrVixwlq+Y8cO7d69W9HR0ZKk6Ohobd682Wk0x+XLl8vPz09NmjSxagqvo6CmYB2enp5q27atU01+fr5WrFhh1QAAAADAxWDrwPbkk09q9erV+vnnn7Vu3Tr17t1bbm5u6t+/v/z9/TVkyBDFx8dr1apVSk1N1eDBgxUdHa2OHTtKkrp27aomTZro/vvv16ZNm7Rs2TL97W9/04gRI+Tl5SVJGj58uH766SeNGTNG27dv15tvvqnExEQ98cQTVh/x8fF6++239d5772nbtm16+OGHdezYMQ0ePLhSXhcAAAAAVwZbf4Ztz5496t+/v/744w/Vrl1bN9xwg7755hvVrl1bkvTKK6+oSpUquvPOO5WTk6PY2Fi9+eab1vPd3Ny0aNEiPfzww4qOjlbVqlUVFxenZ5991qqJiIjQ4sWL9cQTT+jVV19VnTp19M477yg2Ntaq6du3rw4ePKgJEyYoMzNTrVq10tKlS4sMRAIAAAAA5cnW38N2ueF72ACgAvHrDQBgY5fN97ABAAAAwJWKwAYAAAAANkVgAwAAAACbIrABAAAAgE0R2AAAAADApghsAAAAAGBTBDYAAAAAsClbf3E2AACoeI7JfE8ngMuXmXhpfU8nV9gAAAAAwKYIbAAAAABgUwQ2AAAAALApAhsAAAAA2BSBDQAAAABsisAGAAAAADZFYAMAAAAAmyKwAQAAAIBNEdgAAAAAwKYIbAAAAABgUwQ2AAAAALApAhsAAAAA2BSBDQAAAABsisAGAAAAADZFYAMAAAAAmyKwAQAAAIBNEdgAAAAAwKYIbAAAAABgUwQ2AAAAALApAhsAAAAA2BSBDQAAAABsisAGAAAAADZFYAMAAAAAmyKwAQAAAIBNEdgAAAAAwKYIbAAAAABgUwQ2AAAAALApAhsAAAAA2BSBDQAAAABsisAGAAAAADZFYAMAAAAAmyKwAQAAAIBNEdgAAAAAwKYIbAAAAABgUwQ2AAAAALApAhsAAAAA2BSBDQAAAABsisAGAAAAADZFYAMAAAAAmyKwAQAAAIBNEdgAAAAAwKYIbAAAAABgUwQ2AAAAALApAhsAAAAA2BSBrZRmzpyp+vXry9vbW1FRUVq/fn1ltwQAAADgMkVgK4UPP/xQ8fHxmjhxojZu3KiWLVsqNjZWBw4cqOzWAAAAAFyGCGyl8I9//ENDhw7V4MGD1aRJE82ePVu+vr569913K7s1AAAAAJch98pu4FJx6tQppaamavz48da8KlWqKCYmRsnJycU+JycnRzk5OdbjrKwsSVJ2dvbFbRYAIPH/2rI7WdkNAMDFY5e/xQv6MMacs47A5qLff/9deXl5Cg4OdpofHBys7du3F/ucKVOmaPLkyUXmh4eHX5QeAQCF+PtXdgcAABvyn2qv3w9HjhyR/zl+ZxHYLqLx48crPj7eepyfn68///xTNWvWlMPhqMTOrizZ2dkKDw/Xr7/+Kj8/v8pu54rFcbAHjkPl4xjYA8fBHjgO9sBxqBzGGB05ckRhYWHnrCOwuahWrVpyc3PT/v37nebv379fISEhxT7Hy8tLXl5eTvMCAgIuVos4Dz8/P/4nZAMcB3vgOFQ+joE9cBzsgeNgDxyHineuK2sFGHTERZ6enmrbtq1WrFhhzcvPz9eKFSsUHR1diZ0BAAAAuFxxha0U4uPjFRcXp3bt2qlDhw6aMWOGjh07psGDB1d2awAAAAAuQwS2Uujbt68OHjyoCRMmKDMzU61atdLSpUuLDEQCe/Hy8tLEiROL3J6KisVxsAeOQ+XjGNgDx8EeOA72wHGwN4c53ziSAAAAAIBKwWfYAAAAAMCmCGwAAAAAYFMENgAAAACwKQIbAAAAANgUgQ2XvD///FMDBgyQn5+fAgICNGTIEB09evSc9Y899pgiIyPl4+OjunXr6vHHH1dWVpZTncPhKDLNmzfvYu/OJWPmzJmqX7++vL29FRUVpfXr15+zfv78+WrcuLG8vb3VvHlzff75507LjTGaMGGCQkND5ePjo5iYGO3cufNi7sJloTTH4e2339aNN96oGjVqqEaNGoqJiSlSP2jQoCLv+27dul3s3bjkleY4JCQkFHmNvb29nWo4H8qmNMfh5ptvLvb/8z169LBqOB9KZ82aNbrtttsUFhYmh8OhhQsXnvc5SUlJatOmjby8vNSwYUMlJCQUqSnt75srXWmPwyeffKJbbrlFtWvXlp+fn6Kjo7Vs2TKnmkmTJhU5Fxo3bnwR9wKFEdhwyRswYIC2bt2q5cuXa9GiRVqzZo2GDRtWYv3evXu1d+9eTZ8+XVu2bFFCQoKWLl2qIUOGFKmdM2eO9u3bZ029evW6iHty6fjwww8VHx+viRMnauPGjWrZsqViY2N14MCBYuvXrVun/v37a8iQIUpLS1OvXr3Uq1cvbdmyxaqZNm2aXnvtNc2ePVspKSmqWrWqYmNjdfLkyYrarUtOaY9DUlKS+vfvr1WrVik5OVnh4eHq2rWrfvvtN6e6bt26Ob3vP/jgg4rYnUtWaY+DJPn5+Tm9xr/88ovTcs6H0ivtcfjkk0+cjsGWLVvk5uamu+++26mO88F1x44dU8uWLTVz5kyX6jMyMtSjRw917txZ6enpGjlypB588EGnsFCW8+tKV9rjsGbNGt1yyy36/PPPlZqaqs6dO+u2225TWlqaU13Tpk2dzoWvv/76YrSP4hjgEvb9998bSWbDhg3WvCVLlhiHw2F+++03l9eTmJhoPD09zenTp615ksyCBQvKs93LRocOHcyIESOsx3l5eSYsLMxMmTKl2Pp77rnH9OjRw2leVFSUeeihh4wxxuTn55uQkBDz0ksvWcsPHz5svLy8zAcffHAR9uDyUNrjcLbc3FxTvXp1895771nz4uLizB133FHerV7WSnsc5syZY/z9/UtcH+dD2Vzo+fDKK6+Y6tWrm6NHj1rzOB/KzpXfoWPGjDFNmzZ1mte3b18TGxtrPb7Q43qlK+vfMk2aNDGTJ0+2Hk+cONG0bNmy/BpDqXCFDZe05ORkBQQEqF27dta8mJgYValSRSkpKS6vJysrS35+fnJ3d/4u+REjRqhWrVrq0KGD3n33XRm+tlCnTp1SamqqYmJirHlVqlRRTEyMkpOTi31OcnKyU70kxcbGWvUZGRnKzMx0qvH391dUVFSJ67zSleU4nO348eM6ffq0AgMDneYnJSUpKChIkZGRevjhh/XHH3+Ua++Xk7Ieh6NHj6pevXoKDw/XHXfcoa1bt1rLOB9KrzzOh3/961/q16+fqlat6jSf8+HiOd/vhvI4rii9/Px8HTlypMjvhp07dyosLEwNGjTQgAEDtHv37krq8MpDYMMlLTMzU0FBQU7z3N3dFRgYqMzMTJfW8fvvv+u5554rchvls88+q8TERC1fvlx33nmnHnnkEb3++uvl1vul6vfff1deXp6Cg4Od5gcHB5f4mmdmZp6zvuC/pVnnla4sx+FsY8eOVVhYmNMfQ926ddP777+vFStW6MUXX9Tq1avVvXt35eXllWv/l4uyHIfIyEi9++67+vTTT/Wf//xH+fn5uu6667Rnzx5JnA9lcaHnw/r167VlyxY9+OCDTvM5Hy6ukn43ZGdn68SJE+Xy/zmU3vTp03X06FHdc8891ryoqCjrIySzZs1SRkaGbrzxRh05cqQSO71yuJ+/BKh448aN04svvnjOmm3btl3wdrKzs9WjRw81adJEkyZNclr2zDPPWD+3bt1ax44d00svvaTHH3/8grcLVLapU6dq3rx5SkpKchrwol+/ftbPzZs3V4sWLXT11VcrKSlJXbp0qYxWLzvR0dGKjo62Hl933XW69tpr9c9//lPPPfdcJXZ25frXv/6l5s2bq0OHDk7zOR9wpZk7d64mT56sTz/91OkfxLt372793KJFC0VFRalevXpKTEwsdgwAlC+usMGWRo0apW3btp1zatCggUJCQop88Dg3N1d//vmnQkJCzrmNI0eOqFu3bqpevboWLFggDw+Pc9ZHRUVpz549ysnJueD9u5TVqlVLbm5u2r9/v9P8/fv3l/iah4SEnLO+4L+lWeeVrizHocD06dM1depUffHFF2rRosU5axs0aKBatWpp165dF9zz5ehCjkMBDw8PtW7d2nqNOR9K70KOw7FjxzRv3jyX/ujkfChfJf1u8PPzk4+PT7mcX3DdvHnz9OCDDyoxMbHIrapnCwgI0DXXXMO5UEEIbLCl2rVrq3HjxuecPD09FR0drcOHDys1NdV67sqVK5Wfn6+oqKgS15+dna2uXbvK09NT//vf/4oMqV2c9PR01ahRQ15eXuWyj5cqT09PtW3bVitWrLDm5efna8WKFU5XDQqLjo52qpek5cuXW/UREREKCQlxqsnOzlZKSkqJ67zSleU4SGdGH3zuuee0dOlSp89+lmTPnj36448/FBoaWi59X27KehwKy8vL0+bNm63XmPOh9C7kOMyfP185OTm67777zrsdzofydb7fDeVxfsE1H3zwgQYPHqwPPvjA6astSnL06FH9+OOPnAsVpbJHPQEuVLdu3Uzr1q1NSkqK+frrr02jRo1M//79reV79uwxkZGRJiUlxRhjTFZWlomKijLNmzc3u3btMvv27bOm3NxcY4wx//vf/8zbb79tNm/ebHbu3GnefPNN4+vrayZMmFAp+2g38+bNM15eXiYhIcF8//33ZtiwYSYgIMBkZmYaY4y5//77zbhx46z6tWvXGnd3dzN9+nSzbds2M3HiROPh4WE2b95s1UydOtUEBASYTz/91Hz33XfmjjvuMBEREebEiRMVvn+XitIeh6lTpxpPT0/z0UcfOb3vjxw5Yowx5siRI+bJJ580ycnJJiMjw3z55ZemTZs2plGjRubkyZOVso+XgtIeh8mTJ5tly5aZH3/80aSmppp+/foZb29vs3XrVquG86H0SnscCtxwww2mb9++ReZzPpTekSNHTFpamklLSzOSzD/+8Q+TlpZmfvnlF2OMMePGjTP333+/Vf/TTz8ZX19fM3r0aLNt2zYzc+ZM4+bmZpYuXWrVnO+4oqjSHof//ve/xt3d3cycOdPpd8Phw4etmlGjRpmkpCSTkZFh1q5da2JiYkytWrXMgQMHKnz/rkQENlzy/vjjD9O/f39TrVo14+fnZwYPHmz9AWqMMRkZGUaSWbVqlTHGmFWrVhlJxU4ZGRnGmDNfDdCqVStTrVo1U7VqVdOyZUsze/Zsk5eXVwl7aE+vv/66qVu3rvH09DQdOnQw33zzjbXspptuMnFxcU71iYmJ5pprrjGenp6madOmZvHixU7L8/PzzTPPPGOCg4ONl5eX6dKli9mxY0dF7MolrTTHoV69esW+7ydOnGiMMeb48eOma9eupnbt2sbDw8PUq1fPDB06lD+MXFCa4zBy5EirNjg42Nx6661m48aNTuvjfCib0v5/afv27UaS+eKLL4qsi/Oh9Er6/VrwusfFxZmbbrqpyHNatWplPD09TYMGDcycOXOKrPdcxxVFlfY43HTTTeesN+bM1y2EhoYaT09Pc9VVV5m+ffuaXbt2VeyOXcEcxjBOOQAAAADYEZ9hAwAAAACbIrABAAAAgE0R2AAAAADApghsAAAAAGBTBDYAAAAAsCkCGwAAAADYFIENAAAAAGyKwAYAQDkaNGiQHA6Hbr755spuBQBwGSCwAQBQCidPntQ//vEPRUVFyc/PT76+vrrmmmv00EMP6aeffqrs9gAAlxn3ym4AAIBLxaFDh9SlSxelpaVJkqpXr66rr75au3fv1ltvvaXo6OhK7hAAcLnhChsAAC569NFHrbA2evRo/fnnn9q8ebOysrK0evVqRUZGFvu8cePGqWnTpgoICJCHh4fCwsIUFxenffv2WTWZmZkaMGCAQkND5eXlpZCQEP3lL3/R559/LknKy8vT+PHj1aBBA3l7eyswMFDt2rXTSy+9dPF3HABQaQhsAAC4ICsrS4mJiZKkli1b6sUXX5S7+//dqNKpU6cSr7AtXbpUv/32m8LDw9WwYUNlZmbq/fff1x133GHVPPLII5o7d66OHj2qZs2aydPTU0lJSVq/fr0kaebMmZo6dap2796tyMhI1axZU5s3b9bixYsv4l4DACobt0QCAOCCH374Qbm5uZKkG2+8UQ6Hw+Xn/vvf/1bTpk1VpcqZfyd95513NHToUG3YsEE//vijrr76au3cuVOSNHv2bA0YMECStG/fPmVlZUmStXzw4MF6++23JUlHjx7Vtm3bymcHAQC2xBU2AABcYIyxfi5NWJOk9PR0tW/fXtWqVZPD4dDQoUOtZXv37pUk3XbbbZKkuLg4NWzYUD179tR//vMfhYWFSZJ69uwph8Ohd955R1dddZU6d+6s559/XoGBgRe6awAAG+MKGwAALoiMjJS7u7tyc3P19ddfyxjjUnD7+uuvFRcXJ2OMatasqSZNmjhdGcvLy5MkvfDCC7r++uu1bNkybdmyRWvWrNHixYuVlJSkxYsXKzY2Vhs3btT8+fO1adMmpaWlKSkpSQkJCdq1a5eqVat2UfcfAFA5uMIGAIAL/P39dc8990iS0tLS9NRTT1m3SErSl19+qXXr1hV5XkpKinV1bvPmzVq/fr0GDhxYpG7t2rW66aab9Nprr2nlypV66623JElr1qyRJH333XeqXbu2XnjhBS1atEipqamSpP3792vHjh3lu7MAANvgChsAAC56/fXX9f333ys9PV1Tp07Vm2++qfr16+vXX3/VoUOHNGfOnCLPadGihfVz8+bNVbt2bR04cKBI3bhx47RhwwaFh4fL39/fugJX8PzExET9/e9/V506dVS7dm3t3r1bkuTr66urr776YuwuAMAGuMIGAICLAgMDlZycrOnTp6t9+/bKz8/Xjh07VKNGDT344IPq1KlTkefccsstevHFFxUWFqYTJ06ocePGmjVrVpG6vn37ql27dsrOztbmzZsVEBCgfv366YMPPpB0ZhTKbt26KT8/X1u2bJExRn/5y1+0ZMkSBQQEXOxdBwBUEocp/ClqAAAAAIBtcIUNAAAAAGyKwAYAAAAANkVgAwAAAACbIrABAAAAgE0R2AAAAADApghsAAAAAGBTBDYAAAAAsCkCGwAAAADYFIENAAAAAGyKwAYAAAAANkVgAwAAAACbIrABAAAAgE39P4vPzWoyS39qAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "categories = pd.unique(y_train)\n",
    "fig = plt.figure(figsize = (10, 5))\n",
    "colors = {'Non Interacting':'red', 'Interacting':'green'}         \n",
    "labels = list(colors.keys())\n",
    "handles = [plt.Rectangle((0,0),1,1, color=colors[label]) for label in labels]\n",
    "plt.bar(categories, count, color=colors.values())\n",
    "plt.xlabel(\"Class\",fontweight=\"bold\")\n",
    "plt.ylabel(\"Count\",fontweight=\"bold\")\n",
    "plt.title(\"Interacting and Non-Interacting RNA Sequences\\n\",fontweight=\"bold\")\n",
    "plt.legend(handles, labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b28405",
   "metadata": {},
   "source": [
    "# Handling data imbalnce using oversampling on minority class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ccd696bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x-shape : (583926, 17) \n",
      "y-shape : (583926,)\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "x_resampled, y_resampled = Oversampling(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4fdab20d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x-shape : (578673, 17) \n",
      "y-shape : (578673,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from imblearn.over_sampling import ADASYN\n",
    "\n",
    "x_resampled, y_resampled = Oversampling2(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "117fc1f9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "0    291963\n",
      "1    291963\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "count = y_resampled.value_counts()\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2913c95d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "categories = pd.unique(y_resampled)\n",
    "fig = plt.figure(figsize = (10, 5))\n",
    "colors = {'Non Interacting':'red', 'Interacting':'green'}         \n",
    "labels = list(colors.keys())\n",
    "handles = [plt.Rectangle((0,0),1,1, color=colors[label]) for label in labels]\n",
    "plt.bar(categories, count, color=colors.values())\n",
    "plt.xlabel(\"Class\",fontweight=\"bold\")\n",
    "plt.ylabel(\"Count\",fontweight=\"bold\")\n",
    "plt.title(\"Interacting and Non-Interacting RNA Sequences\\n\",fontweight=\"bold\")\n",
    "plt.legend(handles, labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b11446",
   "metadata": {},
   "source": [
    "# Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "473094ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_resampled, y_resampled, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3de81464",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %pip install --upgrade pip\n",
    "# %pip install \\\n",
    "#    --no-binary lightgbm \\\n",
    "#    --config-settings=cmake.define.USE_OPENMP=OFF \\\n",
    "#    'lightgbm>=4.0.0'\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2a900d14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred1 = RandomForest(X_train, y_train,X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5d1e5fe4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred2 = XGBoost(X_train, y_train,X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5e8b0f25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred3 = Pipelining(X_train, y_train,X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c24bd9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred4 = SVM(X_train,y_train,X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2608d8b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013367 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.05, n_estimators=50, num_leaves=31; total time=   2.7s\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013500 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.05, n_estimators=50, num_leaves=31; total time=   2.7s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013398 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.05, n_estimators=50, num_leaves=31; total time=   2.8s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013272 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.05, n_estimators=50, num_leaves=31; total time=   2.7s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013192 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350356, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500194 -> initscore=0.000776\n",
      "[LightGBM] [Info] Start training from score 0.000776\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.05, n_estimators=50, num_leaves=31; total time=   2.7s\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013226 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.05, n_estimators=50, num_leaves=50; total time=   3.9s\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.039277 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.05, n_estimators=50, num_leaves=50; total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013221 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.05, n_estimators=50, num_leaves=50; total time=   3.0s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013399 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.05, n_estimators=50, num_leaves=50; total time=   2.9s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013247 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350356, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500194 -> initscore=0.000776\n",
      "[LightGBM] [Info] Start training from score 0.000776\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.05, n_estimators=50, num_leaves=50; total time=   2.9s\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013184 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.05, n_estimators=50, num_leaves=100; total time=   3.2s\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013306 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.05, n_estimators=50, num_leaves=100; total time=   3.2s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013247 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.05, n_estimators=50, num_leaves=100; total time=   3.3s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013264 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.05, n_estimators=50, num_leaves=100; total time=   3.2s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013267 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350356, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500194 -> initscore=0.000776\n",
      "[LightGBM] [Info] Start training from score 0.000776\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.05, n_estimators=50, num_leaves=100; total time=   3.2s\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013181 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.05, n_estimators=100, num_leaves=31; total time=   4.9s\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013109 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.05, n_estimators=100, num_leaves=31; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013179 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.05, n_estimators=100, num_leaves=31; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013198 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.05, n_estimators=100, num_leaves=31; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013284 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350356, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500194 -> initscore=0.000776\n",
      "[LightGBM] [Info] Start training from score 0.000776\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.05, n_estimators=100, num_leaves=31; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013174 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.05, n_estimators=100, num_leaves=50; total time=   5.3s\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013322 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.05, n_estimators=100, num_leaves=50; total time=   5.6s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013312 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.05, n_estimators=100, num_leaves=50; total time=   5.3s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013274 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.05, n_estimators=100, num_leaves=50; total time=   5.3s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013279 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350356, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500194 -> initscore=0.000776\n",
      "[LightGBM] [Info] Start training from score 0.000776\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.05, n_estimators=100, num_leaves=50; total time=   5.3s\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013306 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.05, n_estimators=100, num_leaves=100; total time=   6.0s\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013242 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.05, n_estimators=100, num_leaves=100; total time=   6.0s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013233 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.05, n_estimators=100, num_leaves=100; total time=   6.1s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013234 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.05, n_estimators=100, num_leaves=100; total time=   6.0s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013408 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350356, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500194 -> initscore=0.000776\n",
      "[LightGBM] [Info] Start training from score 0.000776\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.05, n_estimators=100, num_leaves=100; total time=   6.0s\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013212 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.05, n_estimators=150, num_leaves=31; total time=   6.9s\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013325 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.05, n_estimators=150, num_leaves=31; total time=   6.8s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013240 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.05, n_estimators=150, num_leaves=31; total time=   6.9s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013312 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.05, n_estimators=150, num_leaves=31; total time=   7.3s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013328 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350356, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500194 -> initscore=0.000776\n",
      "[LightGBM] [Info] Start training from score 0.000776\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.05, n_estimators=150, num_leaves=31; total time=   6.9s\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013351 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.05, n_estimators=150, num_leaves=50; total time=   7.6s\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013537 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.05, n_estimators=150, num_leaves=50; total time=   7.6s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013454 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.05, n_estimators=150, num_leaves=50; total time=   7.5s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013335 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.05, n_estimators=150, num_leaves=50; total time=   7.5s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013374 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350356, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500194 -> initscore=0.000776\n",
      "[LightGBM] [Info] Start training from score 0.000776\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.05, n_estimators=150, num_leaves=50; total time=   7.5s\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013243 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.05, n_estimators=150, num_leaves=100; total time=   8.7s\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013345 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.05, n_estimators=150, num_leaves=100; total time=   8.4s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013243 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.05, n_estimators=150, num_leaves=100; total time=   8.6s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013270 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.05, n_estimators=150, num_leaves=100; total time=   8.4s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013235 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350356, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500194 -> initscore=0.000776\n",
      "[LightGBM] [Info] Start training from score 0.000776\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.05, n_estimators=150, num_leaves=100; total time=   8.5s\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013265 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.1, n_estimators=50, num_leaves=31; total time=   2.7s\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013357 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.1, n_estimators=50, num_leaves=31; total time=   2.7s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013251 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.1, n_estimators=50, num_leaves=31; total time=   2.7s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013245 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.1, n_estimators=50, num_leaves=31; total time=   2.6s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013302 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350356, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500194 -> initscore=0.000776\n",
      "[LightGBM] [Info] Start training from score 0.000776\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.1, n_estimators=50, num_leaves=31; total time=   2.6s\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013322 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.1, n_estimators=50, num_leaves=50; total time=   3.0s\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013263 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.1, n_estimators=50, num_leaves=50; total time=   2.9s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013302 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.1, n_estimators=50, num_leaves=50; total time=   2.9s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013522 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.1, n_estimators=50, num_leaves=50; total time=   2.9s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013289 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350356, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500194 -> initscore=0.000776\n",
      "[LightGBM] [Info] Start training from score 0.000776\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.1, n_estimators=50, num_leaves=50; total time=   2.9s\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013263 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.1, n_estimators=50, num_leaves=100; total time=   3.2s\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013252 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.1, n_estimators=50, num_leaves=100; total time=   3.6s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013261 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.1, n_estimators=50, num_leaves=100; total time=   3.3s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013346 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.1, n_estimators=50, num_leaves=100; total time=   3.2s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013202 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350356, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500194 -> initscore=0.000776\n",
      "[LightGBM] [Info] Start training from score 0.000776\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.1, n_estimators=50, num_leaves=100; total time=   3.2s\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013212 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.1, n_estimators=100, num_leaves=31; total time=   4.7s\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013358 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.1, n_estimators=100, num_leaves=31; total time=   4.7s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013289 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.1, n_estimators=100, num_leaves=31; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013248 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.1, n_estimators=100, num_leaves=31; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013297 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350356, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500194 -> initscore=0.000776\n",
      "[LightGBM] [Info] Start training from score 0.000776\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.1, n_estimators=100, num_leaves=31; total time=   4.7s\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013284 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.1, n_estimators=100, num_leaves=50; total time=   5.1s\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013324 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.1, n_estimators=100, num_leaves=50; total time=   5.1s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013485 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.1, n_estimators=100, num_leaves=50; total time=   5.2s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013236 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.1, n_estimators=100, num_leaves=50; total time=   5.2s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013197 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350356, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500194 -> initscore=0.000776\n",
      "[LightGBM] [Info] Start training from score 0.000776\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.1, n_estimators=100, num_leaves=50; total time=   5.3s\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013340 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.1, n_estimators=100, num_leaves=100; total time=   5.7s\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013238 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.1, n_estimators=100, num_leaves=100; total time=   5.8s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013268 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.1, n_estimators=100, num_leaves=100; total time=   5.7s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013165 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.1, n_estimators=100, num_leaves=100; total time=   5.7s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013535 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350356, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500194 -> initscore=0.000776\n",
      "[LightGBM] [Info] Start training from score 0.000776\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.1, n_estimators=100, num_leaves=100; total time=   5.6s\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013234 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.1, n_estimators=150, num_leaves=31; total time=   6.6s\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013220 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.1, n_estimators=150, num_leaves=31; total time=   6.6s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015042 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.1, n_estimators=150, num_leaves=31; total time=   6.9s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013107 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.1, n_estimators=150, num_leaves=31; total time=   6.5s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013254 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350356, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500194 -> initscore=0.000776\n",
      "[LightGBM] [Info] Start training from score 0.000776\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.1, n_estimators=150, num_leaves=31; total time=   6.6s\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013492 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.1, n_estimators=150, num_leaves=50; total time=   7.1s\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013300 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.1, n_estimators=150, num_leaves=50; total time=   7.1s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015084 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.1, n_estimators=150, num_leaves=50; total time=   7.1s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013326 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.1, n_estimators=150, num_leaves=50; total time=   7.1s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013236 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350356, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500194 -> initscore=0.000776\n",
      "[LightGBM] [Info] Start training from score 0.000776\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.1, n_estimators=150, num_leaves=50; total time=   7.0s\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013198 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.1, n_estimators=150, num_leaves=100; total time=   7.6s\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013317 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.1, n_estimators=150, num_leaves=100; total time=   7.6s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013123 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.1, n_estimators=150, num_leaves=100; total time=   7.5s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013375 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.1, n_estimators=150, num_leaves=100; total time=   7.5s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013250 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350356, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500194 -> initscore=0.000776\n",
      "[LightGBM] [Info] Start training from score 0.000776\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.1, n_estimators=150, num_leaves=100; total time=   7.5s\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014555 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.2, n_estimators=50, num_leaves=31; total time=   2.6s\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013365 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.2, n_estimators=50, num_leaves=31; total time=   2.6s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013263 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.2, n_estimators=50, num_leaves=31; total time=   2.6s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013309 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.2, n_estimators=50, num_leaves=31; total time=   2.6s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013274 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350356, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500194 -> initscore=0.000776\n",
      "[LightGBM] [Info] Start training from score 0.000776\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.2, n_estimators=50, num_leaves=31; total time=   2.6s\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013262 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.2, n_estimators=50, num_leaves=50; total time=   2.8s\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014505 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.2, n_estimators=50, num_leaves=50; total time=   2.8s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013230 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.2, n_estimators=50, num_leaves=50; total time=   2.8s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013315 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.2, n_estimators=50, num_leaves=50; total time=   2.8s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013331 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350356, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500194 -> initscore=0.000776\n",
      "[LightGBM] [Info] Start training from score 0.000776\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.2, n_estimators=50, num_leaves=50; total time=   2.8s\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013285 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.2, n_estimators=50, num_leaves=100; total time=   3.1s\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013193 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.2, n_estimators=50, num_leaves=100; total time=   3.1s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013387 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.2, n_estimators=50, num_leaves=100; total time=   3.1s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013284 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.2, n_estimators=50, num_leaves=100; total time=   3.0s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013313 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350356, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500194 -> initscore=0.000776\n",
      "[LightGBM] [Info] Start training from score 0.000776\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.2, n_estimators=50, num_leaves=100; total time=   3.0s\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013373 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.2, n_estimators=100, num_leaves=31; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013327 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.2, n_estimators=100, num_leaves=31; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013198 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.2, n_estimators=100, num_leaves=31; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013254 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.2, n_estimators=100, num_leaves=31; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013280 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350356, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500194 -> initscore=0.000776\n",
      "[LightGBM] [Info] Start training from score 0.000776\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.2, n_estimators=100, num_leaves=31; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013358 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.2, n_estimators=100, num_leaves=50; total time=   4.5s\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013265 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.2, n_estimators=100, num_leaves=50; total time=   4.5s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013392 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.2, n_estimators=100, num_leaves=50; total time=   4.5s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013277 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.2, n_estimators=100, num_leaves=50; total time=   4.5s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013258 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350356, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500194 -> initscore=0.000776\n",
      "[LightGBM] [Info] Start training from score 0.000776\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.2, n_estimators=100, num_leaves=50; total time=   4.6s\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013323 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.2, n_estimators=100, num_leaves=100; total time=   4.9s\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013275 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.2, n_estimators=100, num_leaves=100; total time=   4.7s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013259 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.2, n_estimators=100, num_leaves=100; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013228 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.2, n_estimators=100, num_leaves=100; total time=   5.0s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013328 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350356, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500194 -> initscore=0.000776\n",
      "[LightGBM] [Info] Start training from score 0.000776\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.2, n_estimators=100, num_leaves=100; total time=   5.1s\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013306 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.2, n_estimators=150, num_leaves=31; total time=   5.4s\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013354 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.2, n_estimators=150, num_leaves=31; total time=   5.4s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013341 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.2, n_estimators=150, num_leaves=31; total time=   5.5s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013298 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.2, n_estimators=150, num_leaves=31; total time=   5.4s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013226 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350356, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500194 -> initscore=0.000776\n",
      "[LightGBM] [Info] Start training from score 0.000776\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.2, n_estimators=150, num_leaves=31; total time=   5.4s\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013253 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.2, n_estimators=150, num_leaves=50; total time=   5.6s\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013146 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.2, n_estimators=150, num_leaves=50; total time=   5.7s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013116 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.2, n_estimators=150, num_leaves=50; total time=   5.6s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013146 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.2, n_estimators=150, num_leaves=50; total time=   5.6s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014930 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350356, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500194 -> initscore=0.000776\n",
      "[LightGBM] [Info] Start training from score 0.000776\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.2, n_estimators=150, num_leaves=50; total time=   6.0s\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013540 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.2, n_estimators=150, num_leaves=100; total time=   6.0s\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013415 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.2, n_estimators=150, num_leaves=100; total time=   6.0s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013315 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.2, n_estimators=150, num_leaves=100; total time=   6.1s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013284 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.2, n_estimators=150, num_leaves=100; total time=   6.0s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013317 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350356, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500194 -> initscore=0.000776\n",
      "[LightGBM] [Info] Start training from score 0.000776\n",
      "[CV] END boosting_type=gbdt, learning_rate=0.2, n_estimators=150, num_leaves=100; total time=   6.0s\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013307 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[CV] END boosting_type=dart, learning_rate=0.05, n_estimators=50, num_leaves=31; total time=   6.5s\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013291 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[CV] END boosting_type=dart, learning_rate=0.05, n_estimators=50, num_leaves=31; total time=   6.4s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013170 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[CV] END boosting_type=dart, learning_rate=0.05, n_estimators=50, num_leaves=31; total time=   6.3s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013332 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[CV] END boosting_type=dart, learning_rate=0.05, n_estimators=50, num_leaves=31; total time=   6.4s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013190 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350356, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500194 -> initscore=0.000776\n",
      "[LightGBM] [Info] Start training from score 0.000776\n",
      "[CV] END boosting_type=dart, learning_rate=0.05, n_estimators=50, num_leaves=31; total time=   6.4s\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013489 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[CV] END boosting_type=dart, learning_rate=0.05, n_estimators=50, num_leaves=50; total time=   7.3s\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013227 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[CV] END boosting_type=dart, learning_rate=0.05, n_estimators=50, num_leaves=50; total time=   7.2s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013279 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[CV] END boosting_type=dart, learning_rate=0.05, n_estimators=50, num_leaves=50; total time=   7.2s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013241 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[CV] END boosting_type=dart, learning_rate=0.05, n_estimators=50, num_leaves=50; total time=   7.2s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013328 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350356, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500194 -> initscore=0.000776\n",
      "[LightGBM] [Info] Start training from score 0.000776\n",
      "[CV] END boosting_type=dart, learning_rate=0.05, n_estimators=50, num_leaves=50; total time=   7.2s\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013383 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[CV] END boosting_type=dart, learning_rate=0.05, n_estimators=50, num_leaves=100; total time=   8.5s\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013281 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[CV] END boosting_type=dart, learning_rate=0.05, n_estimators=50, num_leaves=100; total time=   8.5s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013247 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[CV] END boosting_type=dart, learning_rate=0.05, n_estimators=50, num_leaves=100; total time=   8.5s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013265 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[CV] END boosting_type=dart, learning_rate=0.05, n_estimators=50, num_leaves=100; total time=   8.5s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013257 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350356, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500194 -> initscore=0.000776\n",
      "[LightGBM] [Info] Start training from score 0.000776\n",
      "[CV] END boosting_type=dart, learning_rate=0.05, n_estimators=50, num_leaves=100; total time=   8.5s\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013260 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[CV] END boosting_type=dart, learning_rate=0.05, n_estimators=100, num_leaves=31; total time=  22.9s\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013237 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[CV] END boosting_type=dart, learning_rate=0.05, n_estimators=100, num_leaves=31; total time=  22.7s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013309 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[CV] END boosting_type=dart, learning_rate=0.05, n_estimators=100, num_leaves=31; total time=  22.3s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013184 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[CV] END boosting_type=dart, learning_rate=0.05, n_estimators=100, num_leaves=31; total time=  22.6s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013341 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350356, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500194 -> initscore=0.000776\n",
      "[LightGBM] [Info] Start training from score 0.000776\n",
      "[CV] END boosting_type=dart, learning_rate=0.05, n_estimators=100, num_leaves=31; total time=  22.7s\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013344 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[CV] END boosting_type=dart, learning_rate=0.05, n_estimators=100, num_leaves=50; total time=  26.3s\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013153 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[CV] END boosting_type=dart, learning_rate=0.05, n_estimators=100, num_leaves=50; total time=  26.1s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013274 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[CV] END boosting_type=dart, learning_rate=0.05, n_estimators=100, num_leaves=50; total time=  26.2s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013101 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[CV] END boosting_type=dart, learning_rate=0.05, n_estimators=100, num_leaves=50; total time=  26.4s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013403 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350356, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500194 -> initscore=0.000776\n",
      "[LightGBM] [Info] Start training from score 0.000776\n",
      "[CV] END boosting_type=dart, learning_rate=0.05, n_estimators=100, num_leaves=50; total time=  25.9s\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013397 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[CV] END boosting_type=dart, learning_rate=0.05, n_estimators=100, num_leaves=100; total time=  32.2s\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013330 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[CV] END boosting_type=dart, learning_rate=0.05, n_estimators=100, num_leaves=100; total time=  31.3s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013355 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[CV] END boosting_type=dart, learning_rate=0.05, n_estimators=100, num_leaves=100; total time=  31.3s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013238 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[CV] END boosting_type=dart, learning_rate=0.05, n_estimators=100, num_leaves=100; total time=  31.3s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013336 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350356, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500194 -> initscore=0.000776\n",
      "[LightGBM] [Info] Start training from score 0.000776\n",
      "[CV] END boosting_type=dart, learning_rate=0.05, n_estimators=100, num_leaves=100; total time=  31.6s\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013338 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[CV] END boosting_type=dart, learning_rate=0.05, n_estimators=150, num_leaves=31; total time=  49.6s\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013309 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[CV] END boosting_type=dart, learning_rate=0.05, n_estimators=150, num_leaves=31; total time=  49.6s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013313 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[CV] END boosting_type=dart, learning_rate=0.05, n_estimators=150, num_leaves=31; total time=  48.4s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013264 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[CV] END boosting_type=dart, learning_rate=0.05, n_estimators=150, num_leaves=31; total time=  49.5s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013266 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350356, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500194 -> initscore=0.000776\n",
      "[LightGBM] [Info] Start training from score 0.000776\n",
      "[CV] END boosting_type=dart, learning_rate=0.05, n_estimators=150, num_leaves=31; total time=  49.4s\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013275 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[CV] END boosting_type=dart, learning_rate=0.05, n_estimators=150, num_leaves=50; total time=  57.5s\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.023891 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[CV] END boosting_type=dart, learning_rate=0.05, n_estimators=150, num_leaves=50; total time=  57.3s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013251 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[CV] END boosting_type=dart, learning_rate=0.05, n_estimators=150, num_leaves=50; total time=  56.9s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013374 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[CV] END boosting_type=dart, learning_rate=0.05, n_estimators=150, num_leaves=50; total time=  57.1s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013307 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350356, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500194 -> initscore=0.000776\n",
      "[LightGBM] [Info] Start training from score 0.000776\n",
      "[CV] END boosting_type=dart, learning_rate=0.05, n_estimators=150, num_leaves=50; total time=  57.4s\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013209 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[CV] END boosting_type=dart, learning_rate=0.05, n_estimators=150, num_leaves=100; total time= 1.1min\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013317 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[CV] END boosting_type=dart, learning_rate=0.05, n_estimators=150, num_leaves=100; total time= 1.1min\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013389 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[CV] END boosting_type=dart, learning_rate=0.05, n_estimators=150, num_leaves=100; total time= 1.1min\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013291 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[CV] END boosting_type=dart, learning_rate=0.05, n_estimators=150, num_leaves=100; total time= 1.1min\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013287 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350356, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500194 -> initscore=0.000776\n",
      "[LightGBM] [Info] Start training from score 0.000776\n",
      "[CV] END boosting_type=dart, learning_rate=0.05, n_estimators=150, num_leaves=100; total time= 1.1min\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013276 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[CV] END boosting_type=dart, learning_rate=0.1, n_estimators=50, num_leaves=31; total time=   6.4s\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013188 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[CV] END boosting_type=dart, learning_rate=0.1, n_estimators=50, num_leaves=31; total time=   6.3s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013295 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[CV] END boosting_type=dart, learning_rate=0.1, n_estimators=50, num_leaves=31; total time=   6.2s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013294 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[CV] END boosting_type=dart, learning_rate=0.1, n_estimators=50, num_leaves=31; total time=   6.6s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013297 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350356, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500194 -> initscore=0.000776\n",
      "[LightGBM] [Info] Start training from score 0.000776\n",
      "[CV] END boosting_type=dart, learning_rate=0.1, n_estimators=50, num_leaves=31; total time=   6.2s\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013229 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[CV] END boosting_type=dart, learning_rate=0.1, n_estimators=50, num_leaves=50; total time=   7.1s\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013277 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[CV] END boosting_type=dart, learning_rate=0.1, n_estimators=50, num_leaves=50; total time=   7.1s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013234 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[CV] END boosting_type=dart, learning_rate=0.1, n_estimators=50, num_leaves=50; total time=   7.2s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013166 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[CV] END boosting_type=dart, learning_rate=0.1, n_estimators=50, num_leaves=50; total time=   7.2s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013368 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350356, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500194 -> initscore=0.000776\n",
      "[LightGBM] [Info] Start training from score 0.000776\n",
      "[CV] END boosting_type=dart, learning_rate=0.1, n_estimators=50, num_leaves=50; total time=   7.1s\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013260 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[CV] END boosting_type=dart, learning_rate=0.1, n_estimators=50, num_leaves=100; total time=   8.6s\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013203 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[CV] END boosting_type=dart, learning_rate=0.1, n_estimators=50, num_leaves=100; total time=   8.5s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013293 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[CV] END boosting_type=dart, learning_rate=0.1, n_estimators=50, num_leaves=100; total time=   8.4s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013273 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[CV] END boosting_type=dart, learning_rate=0.1, n_estimators=50, num_leaves=100; total time=   8.5s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013429 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350356, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500194 -> initscore=0.000776\n",
      "[LightGBM] [Info] Start training from score 0.000776\n",
      "[CV] END boosting_type=dart, learning_rate=0.1, n_estimators=50, num_leaves=100; total time=   8.4s\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013289 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[CV] END boosting_type=dart, learning_rate=0.1, n_estimators=100, num_leaves=31; total time=  22.2s\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013264 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[CV] END boosting_type=dart, learning_rate=0.1, n_estimators=100, num_leaves=31; total time=  21.6s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013290 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[CV] END boosting_type=dart, learning_rate=0.1, n_estimators=100, num_leaves=31; total time=  21.5s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013297 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[CV] END boosting_type=dart, learning_rate=0.1, n_estimators=100, num_leaves=31; total time=  21.9s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013115 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350356, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500194 -> initscore=0.000776\n",
      "[LightGBM] [Info] Start training from score 0.000776\n",
      "[CV] END boosting_type=dart, learning_rate=0.1, n_estimators=100, num_leaves=31; total time=  21.9s\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013263 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[CV] END boosting_type=dart, learning_rate=0.1, n_estimators=100, num_leaves=50; total time=  25.3s\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013184 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[CV] END boosting_type=dart, learning_rate=0.1, n_estimators=100, num_leaves=50; total time=  24.9s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013280 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[CV] END boosting_type=dart, learning_rate=0.1, n_estimators=100, num_leaves=50; total time=  25.6s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013326 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[CV] END boosting_type=dart, learning_rate=0.1, n_estimators=100, num_leaves=50; total time=  25.2s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013391 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350356, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500194 -> initscore=0.000776\n",
      "[LightGBM] [Info] Start training from score 0.000776\n",
      "[CV] END boosting_type=dart, learning_rate=0.1, n_estimators=100, num_leaves=50; total time=  25.3s\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013253 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[CV] END boosting_type=dart, learning_rate=0.1, n_estimators=100, num_leaves=100; total time=  29.9s\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013313 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[CV] END boosting_type=dart, learning_rate=0.1, n_estimators=100, num_leaves=100; total time=  30.2s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013271 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[CV] END boosting_type=dart, learning_rate=0.1, n_estimators=100, num_leaves=100; total time=  30.1s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013321 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[CV] END boosting_type=dart, learning_rate=0.1, n_estimators=100, num_leaves=100; total time=  30.1s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013279 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350356, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500194 -> initscore=0.000776\n",
      "[LightGBM] [Info] Start training from score 0.000776\n",
      "[CV] END boosting_type=dart, learning_rate=0.1, n_estimators=100, num_leaves=100; total time=  30.1s\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013565 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[CV] END boosting_type=dart, learning_rate=0.1, n_estimators=150, num_leaves=31; total time=  47.0s\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013305 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[CV] END boosting_type=dart, learning_rate=0.1, n_estimators=150, num_leaves=31; total time=  47.0s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013439 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[CV] END boosting_type=dart, learning_rate=0.1, n_estimators=150, num_leaves=31; total time=  46.9s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013276 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[CV] END boosting_type=dart, learning_rate=0.1, n_estimators=150, num_leaves=31; total time=  47.0s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013344 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350356, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500194 -> initscore=0.000776\n",
      "[LightGBM] [Info] Start training from score 0.000776\n",
      "[CV] END boosting_type=dart, learning_rate=0.1, n_estimators=150, num_leaves=31; total time=  47.0s\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013190 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[CV] END boosting_type=dart, learning_rate=0.1, n_estimators=150, num_leaves=50; total time=  55.2s\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013322 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[CV] END boosting_type=dart, learning_rate=0.1, n_estimators=150, num_leaves=50; total time=  54.6s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013241 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[CV] END boosting_type=dart, learning_rate=0.1, n_estimators=150, num_leaves=50; total time=  55.4s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013131 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[CV] END boosting_type=dart, learning_rate=0.1, n_estimators=150, num_leaves=50; total time=  55.5s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013305 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350356, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500194 -> initscore=0.000776\n",
      "[LightGBM] [Info] Start training from score 0.000776\n",
      "[CV] END boosting_type=dart, learning_rate=0.1, n_estimators=150, num_leaves=50; total time=  54.4s\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013252 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[CV] END boosting_type=dart, learning_rate=0.1, n_estimators=150, num_leaves=100; total time= 1.1min\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013303 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[CV] END boosting_type=dart, learning_rate=0.1, n_estimators=150, num_leaves=100; total time= 1.1min\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013106 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[CV] END boosting_type=dart, learning_rate=0.1, n_estimators=150, num_leaves=100; total time= 1.1min\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013271 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[CV] END boosting_type=dart, learning_rate=0.1, n_estimators=150, num_leaves=100; total time= 1.1min\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013305 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350356, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500194 -> initscore=0.000776\n",
      "[LightGBM] [Info] Start training from score 0.000776\n",
      "[CV] END boosting_type=dart, learning_rate=0.1, n_estimators=150, num_leaves=100; total time= 1.1min\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013111 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[CV] END boosting_type=dart, learning_rate=0.2, n_estimators=50, num_leaves=31; total time=   6.4s\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013224 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[CV] END boosting_type=dart, learning_rate=0.2, n_estimators=50, num_leaves=31; total time=   6.1s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013264 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[CV] END boosting_type=dart, learning_rate=0.2, n_estimators=50, num_leaves=31; total time=   6.0s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013210 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[CV] END boosting_type=dart, learning_rate=0.2, n_estimators=50, num_leaves=31; total time=   6.1s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013234 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350356, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500194 -> initscore=0.000776\n",
      "[LightGBM] [Info] Start training from score 0.000776\n",
      "[CV] END boosting_type=dart, learning_rate=0.2, n_estimators=50, num_leaves=31; total time=   6.0s\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013302 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[CV] END boosting_type=dart, learning_rate=0.2, n_estimators=50, num_leaves=50; total time=   7.0s\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013104 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[CV] END boosting_type=dart, learning_rate=0.2, n_estimators=50, num_leaves=50; total time=   6.9s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013224 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[CV] END boosting_type=dart, learning_rate=0.2, n_estimators=50, num_leaves=50; total time=   6.9s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013266 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[CV] END boosting_type=dart, learning_rate=0.2, n_estimators=50, num_leaves=50; total time=   6.9s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013408 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350356, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500194 -> initscore=0.000776\n",
      "[LightGBM] [Info] Start training from score 0.000776\n",
      "[CV] END boosting_type=dart, learning_rate=0.2, n_estimators=50, num_leaves=50; total time=   6.9s\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013268 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[CV] END boosting_type=dart, learning_rate=0.2, n_estimators=50, num_leaves=100; total time=   8.0s\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013360 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[CV] END boosting_type=dart, learning_rate=0.2, n_estimators=50, num_leaves=100; total time=   8.4s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013328 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[CV] END boosting_type=dart, learning_rate=0.2, n_estimators=50, num_leaves=100; total time=   8.1s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013246 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[CV] END boosting_type=dart, learning_rate=0.2, n_estimators=50, num_leaves=100; total time=   8.2s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013304 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350356, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500194 -> initscore=0.000776\n",
      "[LightGBM] [Info] Start training from score 0.000776\n",
      "[CV] END boosting_type=dart, learning_rate=0.2, n_estimators=50, num_leaves=100; total time=   8.2s\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013201 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[CV] END boosting_type=dart, learning_rate=0.2, n_estimators=100, num_leaves=31; total time=  20.8s\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013278 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[CV] END boosting_type=dart, learning_rate=0.2, n_estimators=100, num_leaves=31; total time=  20.9s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013349 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[CV] END boosting_type=dart, learning_rate=0.2, n_estimators=100, num_leaves=31; total time=  20.9s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013281 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[CV] END boosting_type=dart, learning_rate=0.2, n_estimators=100, num_leaves=31; total time=  21.3s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013315 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350356, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500194 -> initscore=0.000776\n",
      "[LightGBM] [Info] Start training from score 0.000776\n",
      "[CV] END boosting_type=dart, learning_rate=0.2, n_estimators=100, num_leaves=31; total time=  21.1s\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013286 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[CV] END boosting_type=dart, learning_rate=0.2, n_estimators=100, num_leaves=50; total time=  24.5s\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013308 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[CV] END boosting_type=dart, learning_rate=0.2, n_estimators=100, num_leaves=50; total time=  24.4s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013613 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[CV] END boosting_type=dart, learning_rate=0.2, n_estimators=100, num_leaves=50; total time=  23.8s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013229 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[CV] END boosting_type=dart, learning_rate=0.2, n_estimators=100, num_leaves=50; total time=  24.2s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013239 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350356, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500194 -> initscore=0.000776\n",
      "[LightGBM] [Info] Start training from score 0.000776\n",
      "[CV] END boosting_type=dart, learning_rate=0.2, n_estimators=100, num_leaves=50; total time=  24.3s\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013257 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[CV] END boosting_type=dart, learning_rate=0.2, n_estimators=100, num_leaves=100; total time=  28.7s\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013206 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[CV] END boosting_type=dart, learning_rate=0.2, n_estimators=100, num_leaves=100; total time=  28.8s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013306 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[CV] END boosting_type=dart, learning_rate=0.2, n_estimators=100, num_leaves=100; total time=  29.7s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013320 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[CV] END boosting_type=dart, learning_rate=0.2, n_estimators=100, num_leaves=100; total time=  28.9s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013344 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350356, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500194 -> initscore=0.000776\n",
      "[LightGBM] [Info] Start training from score 0.000776\n",
      "[CV] END boosting_type=dart, learning_rate=0.2, n_estimators=100, num_leaves=100; total time=  29.1s\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013394 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[CV] END boosting_type=dart, learning_rate=0.2, n_estimators=150, num_leaves=31; total time=  45.7s\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013384 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[CV] END boosting_type=dart, learning_rate=0.2, n_estimators=150, num_leaves=31; total time=  45.4s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013260 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[CV] END boosting_type=dart, learning_rate=0.2, n_estimators=150, num_leaves=31; total time=  45.4s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013224 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[CV] END boosting_type=dart, learning_rate=0.2, n_estimators=150, num_leaves=31; total time=  45.8s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013301 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350356, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500194 -> initscore=0.000776\n",
      "[LightGBM] [Info] Start training from score 0.000776\n",
      "[CV] END boosting_type=dart, learning_rate=0.2, n_estimators=150, num_leaves=31; total time=  45.2s\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013353 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[CV] END boosting_type=dart, learning_rate=0.2, n_estimators=150, num_leaves=50; total time=  52.5s\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013356 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[CV] END boosting_type=dart, learning_rate=0.2, n_estimators=150, num_leaves=50; total time=  53.5s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013628 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[CV] END boosting_type=dart, learning_rate=0.2, n_estimators=150, num_leaves=50; total time=  51.6s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013228 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[CV] END boosting_type=dart, learning_rate=0.2, n_estimators=150, num_leaves=50; total time=  53.1s\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013216 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350356, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500194 -> initscore=0.000776\n",
      "[LightGBM] [Info] Start training from score 0.000776\n",
      "[CV] END boosting_type=dart, learning_rate=0.2, n_estimators=150, num_leaves=50; total time=  51.9s\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013386 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[CV] END boosting_type=dart, learning_rate=0.2, n_estimators=150, num_leaves=100; total time= 1.0min\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013260 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[CV] END boosting_type=dart, learning_rate=0.2, n_estimators=150, num_leaves=100; total time= 1.0min\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013209 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[CV] END boosting_type=dart, learning_rate=0.2, n_estimators=150, num_leaves=100; total time= 1.0min\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013325 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[CV] END boosting_type=dart, learning_rate=0.2, n_estimators=150, num_leaves=100; total time= 1.0min\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013273 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350356, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500194 -> initscore=0.000776\n",
      "[LightGBM] [Info] Start training from score 0.000776\n",
      "[CV] END boosting_type=dart, learning_rate=0.2, n_estimators=150, num_leaves=100; total time= 1.0min\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013378 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.05, n_estimators=50, num_leaves=31; total time=   4.4s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013275 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.05, n_estimators=50, num_leaves=31; total time=   4.4s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013292 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.05, n_estimators=50, num_leaves=31; total time=   4.3s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013587 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.05, n_estimators=50, num_leaves=31; total time=   4.4s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013131 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350356, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500194 -> initscore=0.000776\n",
      "[LightGBM] [Info] Start training from score 0.000776\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.05, n_estimators=50, num_leaves=31; total time=   4.3s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013163 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.05, n_estimators=50, num_leaves=50; total time=   4.8s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013289 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.05, n_estimators=50, num_leaves=50; total time=   4.9s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013262 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.05, n_estimators=50, num_leaves=50; total time=   4.8s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013244 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.05, n_estimators=50, num_leaves=50; total time=   4.9s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013294 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350356, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500194 -> initscore=0.000776\n",
      "[LightGBM] [Info] Start training from score 0.000776\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.05, n_estimators=50, num_leaves=50; total time=   4.8s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013396 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.05, n_estimators=50, num_leaves=100; total time=   5.6s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013279 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.05, n_estimators=50, num_leaves=100; total time=   5.5s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013248 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.05, n_estimators=50, num_leaves=100; total time=   5.6s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013273 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.05, n_estimators=50, num_leaves=100; total time=   5.5s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013251 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350356, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500194 -> initscore=0.000776\n",
      "[LightGBM] [Info] Start training from score 0.000776\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.05, n_estimators=50, num_leaves=100; total time=   5.5s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013242 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.05, n_estimators=100, num_leaves=31; total time=   8.1s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013783 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.05, n_estimators=100, num_leaves=31; total time=   8.0s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013375 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.05, n_estimators=100, num_leaves=31; total time=   8.0s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013202 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.05, n_estimators=100, num_leaves=31; total time=   7.9s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013304 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350356, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500194 -> initscore=0.000776\n",
      "[LightGBM] [Info] Start training from score 0.000776\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.05, n_estimators=100, num_leaves=31; total time=   8.3s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013259 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.05, n_estimators=100, num_leaves=50; total time=   8.9s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013139 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.05, n_estimators=100, num_leaves=50; total time=   8.9s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013229 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.05, n_estimators=100, num_leaves=50; total time=   9.0s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013230 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.05, n_estimators=100, num_leaves=50; total time=   8.9s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013305 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350356, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500194 -> initscore=0.000776\n",
      "[LightGBM] [Info] Start training from score 0.000776\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.05, n_estimators=100, num_leaves=50; total time=   8.8s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013324 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.05, n_estimators=100, num_leaves=100; total time=  10.3s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013269 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.05, n_estimators=100, num_leaves=100; total time=  11.7s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014137 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.05, n_estimators=100, num_leaves=100; total time=  10.7s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013446 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.05, n_estimators=100, num_leaves=100; total time=  10.5s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013356 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350356, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500194 -> initscore=0.000776\n",
      "[LightGBM] [Info] Start training from score 0.000776\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.05, n_estimators=100, num_leaves=100; total time=  15.6s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013284 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.05, n_estimators=150, num_leaves=31; total time=  11.9s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013252 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.05, n_estimators=150, num_leaves=31; total time=  12.3s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013164 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.05, n_estimators=150, num_leaves=31; total time=  11.9s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013437 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.05, n_estimators=150, num_leaves=31; total time=  11.6s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013166 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350356, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500194 -> initscore=0.000776\n",
      "[LightGBM] [Info] Start training from score 0.000776\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.05, n_estimators=150, num_leaves=31; total time=  11.7s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013228 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.05, n_estimators=150, num_leaves=50; total time=  13.0s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013293 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.05, n_estimators=150, num_leaves=50; total time=  12.9s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013284 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.05, n_estimators=150, num_leaves=50; total time=  12.9s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013170 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.05, n_estimators=150, num_leaves=50; total time=  12.9s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013235 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350356, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500194 -> initscore=0.000776\n",
      "[LightGBM] [Info] Start training from score 0.000776\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.05, n_estimators=150, num_leaves=50; total time=  13.1s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013504 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.05, n_estimators=150, num_leaves=100; total time=  14.7s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013211 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.05, n_estimators=150, num_leaves=100; total time=  14.8s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013181 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.05, n_estimators=150, num_leaves=100; total time=  14.9s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013625 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.05, n_estimators=150, num_leaves=100; total time=  14.8s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013225 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350356, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500194 -> initscore=0.000776\n",
      "[LightGBM] [Info] Start training from score 0.000776\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.05, n_estimators=150, num_leaves=100; total time=  15.4s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013253 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.1, n_estimators=50, num_leaves=31; total time=   4.3s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013332 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.1, n_estimators=50, num_leaves=31; total time=   4.3s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013306 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.1, n_estimators=50, num_leaves=31; total time=   4.2s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013354 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.1, n_estimators=50, num_leaves=31; total time=   4.3s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013155 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350356, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500194 -> initscore=0.000776\n",
      "[LightGBM] [Info] Start training from score 0.000776\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.1, n_estimators=50, num_leaves=31; total time=   4.2s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013198 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.1, n_estimators=50, num_leaves=50; total time=   6.2s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013218 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.1, n_estimators=50, num_leaves=50; total time=   4.9s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013225 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.1, n_estimators=50, num_leaves=50; total time=   4.8s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013225 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.1, n_estimators=50, num_leaves=50; total time=   4.7s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013194 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350356, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500194 -> initscore=0.000776\n",
      "[LightGBM] [Info] Start training from score 0.000776\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.1, n_estimators=50, num_leaves=50; total time=   4.6s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013299 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.1, n_estimators=50, num_leaves=100; total time=   5.4s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013291 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.1, n_estimators=50, num_leaves=100; total time=   5.4s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015064 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.1, n_estimators=50, num_leaves=100; total time=   5.3s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013114 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.1, n_estimators=50, num_leaves=100; total time=   5.4s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013174 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350356, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500194 -> initscore=0.000776\n",
      "[LightGBM] [Info] Start training from score 0.000776\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.1, n_estimators=50, num_leaves=100; total time=   5.3s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013386 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.1, n_estimators=100, num_leaves=31; total time=   7.9s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013215 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.1, n_estimators=100, num_leaves=31; total time=   7.8s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013525 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.1, n_estimators=100, num_leaves=31; total time=   7.9s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014002 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.1, n_estimators=100, num_leaves=31; total time=   7.8s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013261 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350356, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500194 -> initscore=0.000776\n",
      "[LightGBM] [Info] Start training from score 0.000776\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.1, n_estimators=100, num_leaves=31; total time=   7.7s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013324 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.1, n_estimators=100, num_leaves=50; total time=   8.8s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013529 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.1, n_estimators=100, num_leaves=50; total time=   8.8s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013404 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.1, n_estimators=100, num_leaves=50; total time=   8.6s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013255 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.1, n_estimators=100, num_leaves=50; total time=   8.7s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013349 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350356, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500194 -> initscore=0.000776\n",
      "[LightGBM] [Info] Start training from score 0.000776\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.1, n_estimators=100, num_leaves=50; total time=   8.6s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013224 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.1, n_estimators=100, num_leaves=100; total time=  10.0s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013291 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.1, n_estimators=100, num_leaves=100; total time=   9.9s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013340 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.1, n_estimators=100, num_leaves=100; total time=   9.9s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013213 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.1, n_estimators=100, num_leaves=100; total time=   9.8s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013254 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350356, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500194 -> initscore=0.000776\n",
      "[LightGBM] [Info] Start training from score 0.000776\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.1, n_estimators=100, num_leaves=100; total time=   9.7s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013185 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.1, n_estimators=150, num_leaves=31; total time=  10.9s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013273 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.1, n_estimators=150, num_leaves=31; total time=  11.1s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013211 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.1, n_estimators=150, num_leaves=31; total time=  10.7s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013319 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.1, n_estimators=150, num_leaves=31; total time=  10.9s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013270 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350356, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500194 -> initscore=0.000776\n",
      "[LightGBM] [Info] Start training from score 0.000776\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.1, n_estimators=150, num_leaves=31; total time=  10.7s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013169 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.1, n_estimators=150, num_leaves=50; total time=  11.9s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013289 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.1, n_estimators=150, num_leaves=50; total time=  12.5s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013212 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.1, n_estimators=150, num_leaves=50; total time=  11.9s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013283 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.1, n_estimators=150, num_leaves=50; total time=  12.0s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013387 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350356, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500194 -> initscore=0.000776\n",
      "[LightGBM] [Info] Start training from score 0.000776\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.1, n_estimators=150, num_leaves=50; total time=  11.9s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013332 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.1, n_estimators=150, num_leaves=100; total time=  13.6s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013361 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.1, n_estimators=150, num_leaves=100; total time=  13.7s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013318 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.1, n_estimators=150, num_leaves=100; total time=  13.7s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013274 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.1, n_estimators=150, num_leaves=100; total time=  13.6s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013293 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350356, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500194 -> initscore=0.000776\n",
      "[LightGBM] [Info] Start training from score 0.000776\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.1, n_estimators=150, num_leaves=100; total time=  13.4s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013363 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.2, n_estimators=50, num_leaves=31; total time=   4.1s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013313 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.2, n_estimators=50, num_leaves=31; total time=   4.2s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013315 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.2, n_estimators=50, num_leaves=31; total time=   4.2s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013281 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.2, n_estimators=50, num_leaves=31; total time=   4.1s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013391 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350356, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500194 -> initscore=0.000776\n",
      "[LightGBM] [Info] Start training from score 0.000776\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.2, n_estimators=50, num_leaves=31; total time=   4.0s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013176 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.2, n_estimators=50, num_leaves=50; total time=   4.6s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013233 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.2, n_estimators=50, num_leaves=50; total time=   4.5s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013228 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.2, n_estimators=50, num_leaves=50; total time=   4.5s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013308 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.2, n_estimators=50, num_leaves=50; total time=   4.5s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013165 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350356, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500194 -> initscore=0.000776\n",
      "[LightGBM] [Info] Start training from score 0.000776\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.2, n_estimators=50, num_leaves=50; total time=   4.5s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013239 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.2, n_estimators=50, num_leaves=100; total time=   5.1s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013286 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.2, n_estimators=50, num_leaves=100; total time=   5.2s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013300 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.2, n_estimators=50, num_leaves=100; total time=   5.1s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013303 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.2, n_estimators=50, num_leaves=100; total time=   5.2s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013233 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350356, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500194 -> initscore=0.000776\n",
      "[LightGBM] [Info] Start training from score 0.000776\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.2, n_estimators=50, num_leaves=100; total time=   5.0s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013327 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.2, n_estimators=100, num_leaves=31; total time=   7.0s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013210 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.2, n_estimators=100, num_leaves=31; total time=   7.4s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013367 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.2, n_estimators=100, num_leaves=31; total time=   7.0s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013563 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.2, n_estimators=100, num_leaves=31; total time=   6.9s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013318 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350356, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500194 -> initscore=0.000776\n",
      "[LightGBM] [Info] Start training from score 0.000776\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.2, n_estimators=100, num_leaves=31; total time=   6.8s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013251 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.2, n_estimators=100, num_leaves=50; total time=   7.7s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013315 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.2, n_estimators=100, num_leaves=50; total time=   7.7s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013374 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.2, n_estimators=100, num_leaves=50; total time=   7.7s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013300 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.2, n_estimators=100, num_leaves=50; total time=   7.7s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013253 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350356, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500194 -> initscore=0.000776\n",
      "[LightGBM] [Info] Start training from score 0.000776\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.2, n_estimators=100, num_leaves=50; total time=   7.6s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013413 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.2, n_estimators=100, num_leaves=100; total time=   8.7s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013284 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.2, n_estimators=100, num_leaves=100; total time=   8.7s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013233 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.2, n_estimators=100, num_leaves=100; total time=   8.9s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013276 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.2, n_estimators=100, num_leaves=100; total time=   8.8s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013302 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350356, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500194 -> initscore=0.000776\n",
      "[LightGBM] [Info] Start training from score 0.000776\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.2, n_estimators=100, num_leaves=100; total time=   8.7s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013397 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.2, n_estimators=150, num_leaves=31; total time=   9.2s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013327 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.2, n_estimators=150, num_leaves=31; total time=   9.3s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013201 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.2, n_estimators=150, num_leaves=31; total time=   9.8s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013592 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.2, n_estimators=150, num_leaves=31; total time=   9.2s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013361 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350356, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500194 -> initscore=0.000776\n",
      "[LightGBM] [Info] Start training from score 0.000776\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.2, n_estimators=150, num_leaves=31; total time=   9.2s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013291 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.2, n_estimators=150, num_leaves=50; total time=  10.3s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013272 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.2, n_estimators=150, num_leaves=50; total time=  10.7s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013334 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.2, n_estimators=150, num_leaves=50; total time=  10.2s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013301 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.2, n_estimators=150, num_leaves=50; total time=  10.0s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013247 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350356, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500194 -> initscore=0.000776\n",
      "[LightGBM] [Info] Start training from score 0.000776\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.2, n_estimators=150, num_leaves=50; total time=  10.3s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013302 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.2, n_estimators=150, num_leaves=100; total time=  11.7s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175245, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013234 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500193 -> initscore=0.000771\n",
      "[LightGBM] [Info] Start training from score 0.000771\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.2, n_estimators=150, num_leaves=100; total time=  11.8s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013293 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.2, n_estimators=150, num_leaves=100; total time=  11.8s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013287 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350355, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500196 -> initscore=0.000782\n",
      "[LightGBM] [Info] Start training from score 0.000782\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.2, n_estimators=150, num_leaves=100; total time=  12.1s\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 175246, number of negative: 175110\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013234 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 350356, number of used features: 17\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500194 -> initscore=0.000776\n",
      "[LightGBM] [Info] Start training from score 0.000776\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[CV] END boosting_type=goss, learning_rate=0.2, n_estimators=150, num_leaves=100; total time=  11.7s\n",
      "[LightGBM] [Info] Number of positive: 219057, number of negative: 218887\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016633 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 437944, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500194 -> initscore=0.000776\n",
      "[LightGBM] [Info] Start training from score 0.000776\n"
     ]
    }
   ],
   "source": [
    "y_pred5= LightGBM(X_train,y_train,X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e9c22fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "[CV] END ................................C=0.001, penalty=l1; total time=   0.1s\n",
      "[CV] END ................................C=0.001, penalty=l1; total time=   0.0s\n",
      "[CV] END ................................C=0.001, penalty=l1; total time=   0.0s\n",
      "[CV] END ................................C=0.001, penalty=l1; total time=   0.0s\n",
      "[CV] END ................................C=0.001, penalty=l1; total time=   0.0s\n",
      "[CV] END ................................C=0.001, penalty=l2; total time=   0.9s\n",
      "[CV] END ................................C=0.001, penalty=l2; total time=   0.7s\n",
      "[CV] END ................................C=0.001, penalty=l2; total time=   0.7s\n",
      "[CV] END ................................C=0.001, penalty=l2; total time=   0.8s\n",
      "[CV] END ................................C=0.001, penalty=l2; total time=   0.7s\n",
      "[CV] END .................................C=0.01, penalty=l1; total time=   0.1s\n",
      "[CV] END .................................C=0.01, penalty=l1; total time=   0.1s\n",
      "[CV] END .................................C=0.01, penalty=l1; total time=   0.1s\n",
      "[CV] END .................................C=0.01, penalty=l1; total time=   0.1s\n",
      "[CV] END .................................C=0.01, penalty=l1; total time=   0.0s\n",
      "[CV] END .................................C=0.01, penalty=l2; total time=   0.6s\n",
      "[CV] END .................................C=0.01, penalty=l2; total time=   0.6s\n",
      "[CV] END .................................C=0.01, penalty=l2; total time=   0.8s\n",
      "[CV] END .................................C=0.01, penalty=l2; total time=   0.7s\n",
      "[CV] END .................................C=0.01, penalty=l2; total time=   0.8s\n",
      "[CV] END ..................................C=0.1, penalty=l1; total time=   0.1s\n",
      "[CV] END ..................................C=0.1, penalty=l1; total time=   0.1s\n",
      "[CV] END ..................................C=0.1, penalty=l1; total time=   0.1s\n",
      "[CV] END ..................................C=0.1, penalty=l1; total time=   0.1s\n",
      "[CV] END ..................................C=0.1, penalty=l1; total time=   0.0s\n",
      "[CV] END ..................................C=0.1, penalty=l2; total time=   0.5s\n",
      "[CV] END ..................................C=0.1, penalty=l2; total time=   0.7s\n",
      "[CV] END ..................................C=0.1, penalty=l2; total time=   0.7s\n",
      "[CV] END ..................................C=0.1, penalty=l2; total time=   0.6s\n",
      "[CV] END ..................................C=0.1, penalty=l2; total time=   0.6s\n",
      "[CV] END ....................................C=1, penalty=l1; total time=   0.1s\n",
      "[CV] END ....................................C=1, penalty=l1; total time=   0.1s\n",
      "[CV] END ....................................C=1, penalty=l1; total time=   0.1s\n",
      "[CV] END ....................................C=1, penalty=l1; total time=   0.1s\n",
      "[CV] END ....................................C=1, penalty=l1; total time=   0.0s\n",
      "[CV] END ....................................C=1, penalty=l2; total time=   0.6s\n",
      "[CV] END ....................................C=1, penalty=l2; total time=   0.8s\n",
      "[CV] END ....................................C=1, penalty=l2; total time=   0.6s\n",
      "[CV] END ....................................C=1, penalty=l2; total time=   0.7s\n",
      "[CV] END ....................................C=1, penalty=l2; total time=   0.7s\n",
      "[CV] END ...................................C=10, penalty=l1; total time=   0.1s\n",
      "[CV] END ...................................C=10, penalty=l1; total time=   0.1s\n",
      "[CV] END ...................................C=10, penalty=l1; total time=   0.0s\n",
      "[CV] END ...................................C=10, penalty=l1; total time=   0.1s\n",
      "[CV] END ...................................C=10, penalty=l1; total time=   0.0s\n",
      "[CV] END ...................................C=10, penalty=l2; total time=   0.6s\n",
      "[CV] END ...................................C=10, penalty=l2; total time=   0.7s\n",
      "[CV] END ...................................C=10, penalty=l2; total time=   0.6s\n",
      "[CV] END ...................................C=10, penalty=l2; total time=   0.6s\n",
      "[CV] END ...................................C=10, penalty=l2; total time=   0.6s\n",
      "[CV] END ..................................C=100, penalty=l1; total time=   0.1s\n",
      "[CV] END ..................................C=100, penalty=l1; total time=   0.1s\n",
      "[CV] END ..................................C=100, penalty=l1; total time=   0.1s\n",
      "[CV] END ..................................C=100, penalty=l1; total time=   0.1s\n",
      "[CV] END ..................................C=100, penalty=l1; total time=   0.0s\n",
      "[CV] END ..................................C=100, penalty=l2; total time=   0.5s\n",
      "[CV] END ..................................C=100, penalty=l2; total time=   0.6s\n",
      "[CV] END ..................................C=100, penalty=l2; total time=   0.6s\n",
      "[CV] END ..................................C=100, penalty=l2; total time=   0.6s\n",
      "[CV] END ..................................C=100, penalty=l2; total time=   0.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:547: FitFailedWarning: \n",
      "30 fits failed out of a total of 60.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "30 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 895, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/base.py\", line 1474, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py\", line 1172, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py\", line 67, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or None penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/model_selection/_search.py:1051: UserWarning: One or more of the test scores are non-finite: [       nan 0.56766758        nan 0.56767219        nan 0.56767219\n",
      "        nan 0.56767219        nan 0.56767219        nan 0.56767219]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "y_pred6 = LogisticRegressionModel(X_train,y_train,X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80df5dc2",
   "metadata": {},
   "source": [
    "# Calculating Accuracy and MCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "84bc0f04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import matthews_corrcoef, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eb696c72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matthews-Coefficient =  0.758102043894307\n",
      "Accuracy =  0.8779420608423366\n"
     ]
    }
   ],
   "source": [
    "# for RandomForest\n",
    "print(\"Matthews-Coefficient = \", matthews_corrcoef(y_test, y_pred1))\n",
    "print(\"Accuracy = \",accuracy_score(y_test, y_pred1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "efbc5b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matthews-Coefficient =  0.6084208868707983\n",
      "Accuracy =  0.8040838050999177\n"
     ]
    }
   ],
   "source": [
    "# for XGBoost\n",
    "print(\"Matthews-Coefficient = \", matthews_corrcoef(y_test, y_pred2))\n",
    "print(\"Accuracy = \",accuracy_score(y_test, y_pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1d7ed059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matthews-Coefficient =  0.7747102065441066\n",
      "Accuracy =  0.8872045842578576\n"
     ]
    }
   ],
   "source": [
    "# for RandomForest + XGBoost\n",
    "print(\"Matthews-Coefficient = \", matthews_corrcoef(y_test, y_pred3))\n",
    "print(\"Accuracy = \",accuracy_score(y_test, y_pred3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9e37f761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matthews-Coefficient =  0.4733346440537104\n",
      "Accuracy =  0.6842487429957118\n"
     ]
    }
   ],
   "source": [
    "# for SVM\n",
    "print(\"Matthews-Coefficient = \", matthews_corrcoef(y_test, y_pred4))\n",
    "print(\"Accuracy = \",accuracy_score(y_test, y_pred4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "62989844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matthews-Coefficient =  0.6061149364533069\n",
      "Accuracy =  0.8028318559822444\n"
     ]
    }
   ],
   "source": [
    "# for LGBM\n",
    "print(\"Matthews-Coefficient = \", matthews_corrcoef(y_test, y_pred5))\n",
    "print(\"Accuracy = \",accuracy_score(y_test, y_pred5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3a355ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matthews-Coefficient =  0.14049050834222984\n",
      "Accuracy =  0.5703295108143417\n"
     ]
    }
   ],
   "source": [
    "# for Logistic Regression\n",
    "print(\"Matthews-Coefficient = \", matthews_corrcoef(y_test, y_pred6))\n",
    "print(\"Accuracy = \",accuracy_score(y_test, y_pred6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afde913",
   "metadata": {},
   "source": [
    "# Confusion Matrix for  RandomForest + XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "42e9d9cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 427.9555555555555, 'Predicted label')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "cc=confusion_matrix(y_test, y_pred3)\n",
    "\n",
    "\n",
    "class_names=[0,1] # name  of classes\n",
    "fig, ax = plt.subplots()\n",
    "tick_marks = np.arange(len(class_names))\n",
    "plt.xticks(tick_marks, class_names)\n",
    "plt.yticks(tick_marks, class_names)\n",
    "# create heatmap\n",
    "sns.heatmap(pd.DataFrame(cc), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "plt.tight_layout()\n",
    "plt.title('Confusion matrix', y=1.1)\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595bcf98",
   "metadata": {},
   "source": [
    "# Precesion Recall Curve for RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fe27f43b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_pred1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m precision_recall_curve\n\u001b[0;32m----> 3\u001b[0m precision, recall, thresholds \u001b[38;5;241m=\u001b[39m precision_recall_curve(y_test, \u001b[43my_pred1\u001b[49m)\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m5\u001b[39m,\u001b[38;5;241m5\u001b[39m))\n\u001b[1;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(recall, precision, label \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRand\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_pred1' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_pred1)\n",
    "plt.figure(figsize = (5,5))\n",
    "plt.plot(recall, precision, label = 'Rand')\n",
    "plt.xlabel('recall',fontweight=\"bold\")\n",
    "plt.ylabel('precision',fontweight=\"bold\")\n",
    "plt.title('Hybrid Model PRC Curve',fontweight=\"bold\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79986598",
   "metadata": {},
   "source": [
    "# Precision Recall Curve for RandomForest + XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4da81db9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_pred3)\n",
    "plt.figure(figsize = (5,5))\n",
    "plt.plot(recall, precision, label = 'Rand')\n",
    "plt.xlabel('recall',fontweight=\"bold\")\n",
    "plt.ylabel('precision',fontweight=\"bold\")\n",
    "plt.title('Hybrid Model PRC Curve',fontweight=\"bold\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e8f8aa",
   "metadata": {},
   "source": [
    "# Precision Recall Curve for LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2a70d3a1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_pred5' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m precision_recall_curve\n\u001b[0;32m----> 3\u001b[0m precision, recall, thresholds \u001b[38;5;241m=\u001b[39m precision_recall_curve(y_test, \u001b[43my_pred5\u001b[49m)\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m5\u001b[39m,\u001b[38;5;241m5\u001b[39m))\n\u001b[1;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(recall, precision, label \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRand\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_pred5' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_pred5)\n",
    "plt.figure(figsize = (5,5))\n",
    "plt.plot(recall, precision, label = 'Rand')\n",
    "plt.xlabel('recall',fontweight=\"bold\")\n",
    "plt.ylabel('precision',fontweight=\"bold\")\n",
    "plt.title('Hybrid Model PRC Curve',fontweight=\"bold\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3daa4a",
   "metadata": {},
   "source": [
    "# ROC Curve for RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0fec188e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score: 0.8782043502309718\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred1)\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.plot(fpr, tpr, label='Rand')\n",
    "plt.xlabel('FPR', fontweight=\"bold\")\n",
    "plt.ylabel('TPR', fontweight=\"bold\")\n",
    "plt.title('Hybrid Model ROC curve\\n', fontweight=\"bold\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "auc = roc_auc_score(y_test, y_pred1)\n",
    "print(\"AUC Score:\", auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2500f2",
   "metadata": {},
   "source": [
    "# Receiver Operating Characterics  Curve for RandomForest + XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d5cc27a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred3)\n",
    "plt.figure(figsize = (5,5))\n",
    "plt.plot(fpr, tpr, label = 'Rand')\n",
    "plt.xlabel('FPR',fontweight=\"bold\")\n",
    "plt.ylabel('TPR ',fontweight=\"bold\")\n",
    "plt.title('Hybrid Model ROC curve\\n',fontweight=\"bold\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba560ba",
   "metadata": {},
   "source": [
    "# Training Hybrid Model on entire train data and finding the predictions for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "80a30d5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred = Pipelining(x_resampled, y_resampled,test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3eee554",
   "metadata": {},
   "source": [
    "# Generate CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "71bffd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "GenerateCSV(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f95fb0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.11.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
